{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "穿着背心的*醒来，看看手机，三点了。\n"
     ]
    }
   ],
   "source": [
    "def read_list(text_path):\n",
    "    lsit=[]\n",
    "    with open('%s' % text_path, 'r', encoding=\"utf8\") as f:  # 打开一个文件只读模式\n",
    "        line = f.readlines()  # 读取文件中的每一行，放入line列表中\n",
    "        for line_list in line:\n",
    "            lsit.append(line_list.replace('\\n',''))\n",
    "    return lsit\n",
    "train_x=read_list('../data/pre_data.txt')\n",
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ban是cls的后缀\n",
    "ban='cls_1637069358.4556513_3_0.6593_0.64'\n",
    "cuda_num = str(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "        self.model.to(device)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, 24)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "        x = outputs[1]\n",
    "        x = self.dropout(x)\n",
    "        x = self.l1(x)#8 24\n",
    "        end=[]\n",
    "        for i in range(len(x)):\n",
    "            end.append(torch.stack(list(x[i].split(4, 0))))\n",
    "        x=torch.stack(end)\n",
    "#         print(x)\n",
    "        return x #batchsize* 6 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device_s = \"cuda:\" + cuda_num\n",
    "device = torch.device(device_s if torch.cuda.is_available() else \"cpu\")\n",
    "cls=torch.load(\"../data/\"+str(ban)+\".model\",map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Softmax=nn.Softmax(dim = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(text):\n",
    "    text2id = tokenizer(\n",
    "        text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids=text2id[\"input_ids\"].to(device)\n",
    "    mask=text2id[\"attention_mask\"].to(device)\n",
    "    output = cls(input_ids, attention_mask=mask)\n",
    "    output=Softmax(output)\n",
    "#     print(output)\n",
    "    output0=output.argmax(dim=2)\n",
    "\n",
    "#     output0=_18to6_(output,round_=1)\n",
    "#     print(output0)\n",
    "    output1=[]\n",
    "    \n",
    "    for i in output0:#18\n",
    "        s=''\n",
    "        \n",
    "        for j in i:\n",
    "            s+=str(j.item())\n",
    "            s+=','\n",
    "        s=s[:-1]\n",
    "    \n",
    "        output1.append(s)\n",
    "        \n",
    "    return output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0,0,1,0,0,0', '0,0,1,0,0,0']\n"
     ]
    }
   ],
   "source": [
    "text = ['*与n3：啊？','*与n3：啊？']\n",
    "# 爱、乐、惊、怒、恐、哀\n",
    "print(get_output(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 21376\r"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "\n",
    "i=0\n",
    "batch_size=16\n",
    "sum_=0\n",
    "while(i<len(train_x)):\n",
    "    if i+batch_size<=len(train_x):\n",
    "        input_x=train_x[i:i+batch_size]\n",
    "#         print('{}:{},len={}'.format(i,i+batch_size,batch_size),end = \"\\r\")\n",
    "    else:\n",
    "        input_x=train_x[i:len(train_x)]\n",
    "#         print('{}:{},len={}'.format(i,len(train_x)-1,len(train_x)-i),end = \"\\r\")\n",
    "    \n",
    "    sum_+=len(input_x)\n",
    "    print(len(input_x),sum_,end = \"\\r\")\n",
    "    pre=get_output(input_x)\n",
    "#     print(pre)\n",
    "    result+=pre\n",
    "#     print(len(result))\n",
    "    i+=batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34170_0002_A_12\n",
      "21376\n",
      "21376\n"
     ]
    }
   ],
   "source": [
    "end_list=read_list('../data/end_list.txt')\n",
    "print(end_list[0])\n",
    "print(len(result))\n",
    "print(len(end_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = open('../data/result'+str(ban).replace('cls','')+'.txt', encoding='utf-8', mode='w')\n",
    "ff.write('id\\temotion\\n')\n",
    "for i in range(len(end_list)):\n",
    "    ff.write(end_list[i]+'\\t'+result[i])\n",
    "    ff.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs0",
   "language": "python",
   "name": "bs0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
