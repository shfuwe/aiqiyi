{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ban=0\n",
    "batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device0 = torch.device('cuda:5' if torch.cuda.is_available() else \"cpu\")\n",
    "device1 = torch.device('cuda:5' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读数据\n",
    "import pandas as pd\n",
    "df_train=pd.read_excel(\"../data/df_train.xlsx\",index_col=0)\n",
    "df_test=pd.read_excel(\"../data/df_test.xlsx\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "label18=[]\n",
    "for i in range(len(df_train)):\n",
    "    new_list=[]\n",
    "    old_list=ast.literal_eval(df_train.loc[i]['label'])\n",
    "    df_train.loc[i]['label']=old_list\n",
    "    for j in old_list:\n",
    "        if j==0:\n",
    "            new_list+=[0,0,0]\n",
    "        if j==1:\n",
    "            new_list+=[0,0,1]\n",
    "        if j==2:\n",
    "            new_list+=[0,1,1]\n",
    "        if j==3:\n",
    "            new_list+=[1,1,1]\n",
    "    label18.append(new_list)\n",
    "\n",
    "df_train['label18']=label18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label18=[]\n",
    "for i in range(len(df_test)):\n",
    "    new_list=[]\n",
    "    old_list=ast.literal_eval(df_test.loc[i]['label'])\n",
    "    df_test.loc[i]['label']=old_list\n",
    "    for j in old_list:\n",
    "        if j==0:\n",
    "            new_list+=[0,0,0]\n",
    "        if j==1:\n",
    "            new_list+=[0,0,1]\n",
    "        if j==2:\n",
    "            new_list+=[0,1,1]\n",
    "        if j==3:\n",
    "            new_list+=[1,1,1]\n",
    "    label18.append(new_list)\n",
    "\n",
    "df_test['label18']=label18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "df_train_text=[df_train['text'][i] for i in range(len(df_train))]\n",
    "df_test_text=[df_test['text'][i] for i in range(len(df_test))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2id_train = tokenizer(\n",
    "        df_train_text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "input_ids_train=text2id_train[\"input_ids\"]\n",
    "mask_train=text2id_train[\"attention_mask\"]\n",
    "\n",
    "text2id_test = tokenizer(\n",
    "        df_test_text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "input_ids_test=text2id_test[\"input_ids\"]\n",
    "mask_test=text2id_test[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['input_ids']=input_ids_train.tolist()\n",
    "df_train['mask']=mask_train.tolist()\n",
    "\n",
    "df_test['input_ids']=input_ids_test.tolist()\n",
    "df_test['mask']=mask_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.dataset = df\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"text\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "#         print(label)\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        mask = self.dataset.loc[idx, \"mask\"]\n",
    "        label18=self.dataset.loc[idx, \"label18\"]\n",
    "        sample = {\"text\": text, \"label\": label,\"label18\": label18,\"input_ids\":input_ids,\"mask\":mask}\n",
    "        # print(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fc65b65cad0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7fc4fa0d8310>\n"
     ]
    }
   ],
   "source": [
    "#按batch_size分\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    SentimentDataset(df_train), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "print(train_loader)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    SentimentDataset(df_test), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "        self.model.to(device)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, 18)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "        x = outputs[1]\n",
    "        x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if ban==0:\n",
    "    cls = fn_cls(device0)\n",
    "    cls.train()\n",
    "else:\n",
    "    cls=torch.load(\"../data/cls\"+str(ban)+\".model\",map_location=device0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _18to6_(output,round_=0):#18tensor 变 6list\n",
    "    output0=[]\n",
    "    for j in output:#18\n",
    "        i=0\n",
    "        s=''\n",
    "        list6=[]\n",
    "        while(i<len(j)):\n",
    "            ok=j[i]*1.2+j[i+1]*1.1+j[i+2]*1\n",
    "            if round_==1:\n",
    "                list6.append(ok.round().int().tolist())\n",
    "            else:\n",
    "                list6.append(ok.tolist())\n",
    "            i+=3\n",
    "        output0.append(list6)\n",
    "    return output0\n",
    "\n",
    "def get_loss_test(output,A,round_=0):#8*18\n",
    "    output_r=torch.Tensor(_18to6_(output,round_=1)).to(device0)\n",
    "    output=_18to6_(output,round_=round_)\n",
    "    # print(output_r,A)\n",
    "    cor_add=(output_r == A).sum().item()\n",
    "    sum0=0\n",
    "    for i in range(len(output)):\n",
    "        for j in range(6):\n",
    "            sum0+=(output[i][j]-A[i][j])*(output[i][j]-A[i][j])\n",
    "    return sum0,cor_add\n",
    "\n",
    "def test(device_test):\n",
    "    cls.to(device_test)\n",
    "    cls.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_test=0\n",
    "    for batch_idx,batch in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            # print(batch['label'],batch['label18'])\n",
    "            label6=torch.stack(batch['label']).t().to(device_test).float()\n",
    "            # label18=torch.stack(batch['label18']).t().to(device_test).float()\n",
    "\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device_test)\n",
    "            mask=torch.stack(batch['mask']).t().to(device_test)\n",
    "\n",
    "            output = cls(input_ids, attention_mask=mask)\n",
    "            output=sigmoid(output)\n",
    "\n",
    "            total += len(output)*6\n",
    "            loss_add,cor_add = get_loss_test(output, label6 ,round_=1)\n",
    "            loss_test+=loss_add\n",
    "            correct+=cor_add\n",
    "\n",
    "            tes_score=1/(1+(loss_test/total) ** 0.5)\n",
    "            acc_score=100.*correct/total\n",
    "\n",
    "        \n",
    "\n",
    "        print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{:.2f}%，score：{}'.format(\n",
    "                    batch_idx, len(test_loader),100.*batch_idx/len(test_loader), \n",
    "                    correct, total,acc_score,\n",
    "                    tes_score\n",
    "            ),end='\\r')\n",
    "    print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{:.2f}%，score：{}'.format(\n",
    "                    batch_idx, len(test_loader),100.*batch_idx/len(test_loader), \n",
    "                    correct, total,acc_score,\n",
    "                    tes_score\n",
    "            ))\n",
    "#     cls.to(device_train)\n",
    "    return tes_score,acc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24585.0\n",
      "[408.0, 722.0, 1132.0, 221.0, 646.0, 1868.0, 280.0, 752.0, 1566.0, 544.0, 1589.0, 2994.0, 378.0, 1202.0, 2130.0, 932.0, 2638.0, 4583.0]\n"
     ]
    }
   ],
   "source": [
    "weight=[]\n",
    "import numpy as np\n",
    "sumn=np.zeros(18)\n",
    "for i in range(len(df_train)):\n",
    "    print(i,end='\\r')\n",
    "    sumn+=np.array(df_train.loc[i]['label18'])\n",
    "#     print(sumn,end='\\r')\n",
    "\n",
    "print(sumn.sum())\n",
    "print(sumn.tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.232843137254902, 6.347645429362881, 4.048586572438163, 20.737556561085974, 7.094427244582043, 2.4534261241970023, 16.367857142857144, 6.094414893617022, 2.926564495530013, 8.424632352941176, 2.8842039018250474, 1.5307281229124916, 12.124338624338625, 3.812811980033278, 2.151643192488263, 4.917381974248927, 1.7373009855951478, 1.0]\n",
      "tensor([11.2328,  6.3476,  4.0486, 20.7376,  7.0944,  2.4534, 16.3679,  6.0944,\n",
      "         2.9266,  8.4246,  2.8842,  1.5307, 12.1243,  3.8128,  2.1516,  4.9174,\n",
      "         1.7373,  1.0000], device='cuda:5')\n"
     ]
    }
   ],
   "source": [
    "sumn=sumn.tolist()\n",
    "weight=[max(sumn)/i for i in sumn]\n",
    "print(weight)\n",
    "weight=torch.Tensor(weight).to(device0)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "#BCEWithLogitsLoss就是把Sigmoid-BCELoss合成一步\n",
    "criterion = nn.BCELoss(weight=weight)\n",
    "criterion_hui=nn.MSELoss()\n",
    "optimizer = optim.Adam(cls.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:3 start _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "labels: tensor([[0., 0., 0., 0., 0., 0.],618769\t准确率：87.79%\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 0., 0.],\n",
      "        [0., 0., 0., 3., 0., 0.]], device='cuda:5')\n",
      "pred: tensor([[0.0614, 0.0628, 0.0742, 0.3429, 0.2000, 0.2651],\n",
      "        [0.1109, 0.1388, 0.1099, 0.3352, 0.1373, 0.3338],\n",
      "        [0.0673, 0.0557, 0.0699, 0.1563, 0.1175, 0.4263],\n",
      "        [0.0311, 0.1244, 0.0510, 0.1608, 0.1645, 0.3790],\n",
      "        [0.0408, 0.0584, 0.0403, 0.2106, 0.0909, 0.1391],\n",
      "        [0.0336, 0.2584, 0.0983, 0.2128, 0.2148, 0.3255]], device='cuda:5')\n",
      "Train Epoch: 3\tscore:0.618994\t准确率：87.81%\n",
      "[834/835 (100%)]\t正确分类的样本数：35577，样本总数：40080，准确率：88.76%，score：0.6916372179985046\n",
      "epoch: 3_执行时间:  299.3692343235016\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:3 end _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:4 start _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "labels: tensor([[0., 1., 0., 0., 0., 0.],663052\t准确率：92.10%\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [3., 0., 0., 0., 0., 3.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.]], device='cuda:5')\n",
      "pred: tensor([[0.0859, 0.0913, 0.1078, 0.1617, 0.1225, 0.2532],\n",
      "        [0.0859, 0.0913, 0.1078, 0.1617, 0.1225, 0.2532],\n",
      "        [0.0859, 0.0913, 0.1078, 0.1617, 0.1225, 0.2532],\n",
      "        [0.0859, 0.0913, 0.1078, 0.1617, 0.1225, 0.2532],\n",
      "        [0.0859, 0.0913, 0.1078, 0.1617, 0.1225, 0.2532],\n",
      "        [0.0859, 0.0913, 0.1078, 0.1617, 0.1225, 0.2532]], device='cuda:5')\n",
      "Train Epoch: 4\tscore:0.663110\t准确率：92.10%\n",
      "[834/835 (100%)]\t正确分类的样本数：35577，样本总数：40080，准确率：88.76%，score：0.6916372179985046\n",
      "epoch: 4_执行时间:  294.396767616272\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:4 end _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:5 start _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "labels: tensor([[0., 0., 0., 0., 0., 0.],662932\t准确率：92.09%\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 3., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.]], device='cuda:5')\n",
      "pred: tensor([[0.0871, 0.0863, 0.0904, 0.1889, 0.1132, 0.2644],\n",
      "        [0.0871, 0.0863, 0.0904, 0.1889, 0.1132, 0.2644],\n",
      "        [0.0871, 0.0863, 0.0904, 0.1889, 0.1132, 0.2644],\n",
      "        [0.0871, 0.0863, 0.0904, 0.1889, 0.1132, 0.2644],\n",
      "        [0.0871, 0.0863, 0.0904, 0.1889, 0.1132, 0.2644],\n",
      "        [0.0871, 0.0863, 0.0904, 0.1889, 0.1132, 0.2644]], device='cuda:5')\n",
      "Train Epoch: 5\tscore:0.663048\t准确率：92.10%\n",
      "[834/835 (100%)]\t正确分类的样本数：35577，样本总数：40080，准确率：88.76%，score：0.6916372179985046\n",
      "epoch: 5_执行时间:  295.7029507160187\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:5 end _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:6 start _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Train Epoch: 6 [1984/3763 (53%)]\tscore:0.661507\t准确率：91.97%\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23094/1561508042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m#     plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_be\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_23094/1561508042.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device_train, device_test, epoch_be)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlossh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m#计算score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bs0/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bs0/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time  # 引入time模块\n",
    " \n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(device_train,device_test,epoch_be=0):\n",
    "    epoch_num=4\n",
    "    accumulation_steps=32\n",
    "    \n",
    "    train_score=[]\n",
    "    test_score=[]\n",
    "    test_acc=[]\n",
    "\n",
    "    for epoch_idx in range(epoch_be,epoch_be+epoch_num):\n",
    "        cls.to(device_train)\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________ epoch:\"+str(epoch_idx)+\" start _________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        score_list=[]\n",
    "        loss_a=0\n",
    "        total=0\n",
    "        correct=0\n",
    "        start = time.time()\n",
    "        for batch_idx,batch in enumerate(train_loader):\n",
    "            label6=torch.stack(batch['label']).t().to(device_test).float()\n",
    "            label18=torch.stack(batch['label18']).t().to(device_test).float()\n",
    "\n",
    "            #计算output\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device_train)\n",
    "            mask=torch.stack(batch['mask']).t().to(device_train)\n",
    "            output = cls(input_ids, attention_mask=mask)\n",
    "            output=sigmoid(output)\n",
    "            output6=torch.Tensor(_18to6_(output,round_=0)).to(device_train)\n",
    "\n",
    "\n",
    "            # 梯度积累\n",
    "            loss0 = criterion(output, label18)\n",
    "            lossh=criterion_hui(output6,label6)\n",
    "\n",
    "            loss=loss0+lossh\n",
    "            loss = loss/accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            #计算score\n",
    "            with torch.no_grad():\n",
    "                total += len(output)*6\n",
    "                loss_add,cor_add = get_loss_test(output, label6,round_=0)\n",
    "                loss_a+=loss_add\n",
    "                correct+=cor_add\n",
    "\n",
    "                tr_score=1/(1+(loss_a/total) ** 0.5)\n",
    "                acc_score=100.*correct/total\n",
    "\n",
    "\n",
    "            if((batch_idx+1) % accumulation_steps) == 0:\n",
    "                # 每 accumulation_steps 次更新一下网络中的参数\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if ((batch_idx+1) % accumulation_steps) == 1:\n",
    "                score_list.append(tr_score)\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tscore:{:.6f}\\t准确率：{:.2f}%'.format(\n",
    "                    epoch_idx, batch_idx, len(train_loader), \n",
    "                    100.*batch_idx/len(train_loader), tr_score,acc_score\n",
    "                ),end='\\r')\n",
    "\n",
    "        #每个epoch结束：\n",
    "        print('labels:', label6)\n",
    "        print('pred:', output6)\n",
    "#         plt.plot([i for i in range(len(score_list))], score_list)\n",
    "#         plt.show()\n",
    "\n",
    "        print('Train Epoch: {}\\tscore:{:.6f}\\t准确率：{:.2f}%'.format(epoch_idx,tr_score,acc_score))\n",
    "        t_score,t_acc=test(device_test)\n",
    "\n",
    "        train_score.append(tr_score)\n",
    "        test_score.append(t_score)\n",
    "        test_acc.append(t_acc)\n",
    "\n",
    "        #保存模型\n",
    "        end = time.time()\n",
    "        print('epoch:',str(epoch_idx)+'_执行时间: ',end - start)\n",
    "        torch.save(cls,\"../data/cls_\"+str(end)+\"_\"+str(epoch_idx)+\"_\"+str(round(tr_score.tolist(),4))+\"_\"+str(round(t_score.tolist(),4))+\".model\")\n",
    "\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________ epoch:\"+str(epoch_idx)+\" end _________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "\n",
    "#     plt.plot([i for i in range(len(train_score))], train_score)\n",
    "#     plt.show()             \n",
    "\n",
    "#     plt.plot([i for i in range(len(test_score))], test_score)\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.plot([i for i in range(len(test_acc))], test_acc)\n",
    "#     plt.show()\n",
    "    \n",
    "train(device0,device1,epoch_be=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "torch.save(cls,\"../data/cls_\"+str(end)+\"_\"+str(2)+\"_\"+str(0.6886)+\"_\"+str(0.6942)+\".model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "def get_output(device_pre,model,text,f=0):\n",
    "    model.to(device_pre)\n",
    "    model.eval()\n",
    "    text2id = tokenizer(\n",
    "        text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "#     print(text,text2id)\n",
    "    input_ids=text2id[\"input_ids\"].to(device_pre)\n",
    "    mask=text2id[\"attention_mask\"].to(device_pre)\n",
    "#         print(text2id)\n",
    "    output = model(input_ids, attention_mask=mask)\n",
    "    print(output)\n",
    "    output=sigmoid(output)\n",
    "    print(output)\n",
    "    \n",
    "#     output=sigmoid(output)*4\n",
    "    if f==1:\n",
    "        return _18to6_(output,round_=1)\n",
    "    else:\n",
    "        return _18to6_(output,round_=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.3721, -3.8519, -3.1818, -4.7673, -3.8145, -2.7889, -4.2150, -3.6298,\n",
      "         -2.7937, -4.0326, -2.8436, -2.0866, -4.4879, -3.0374, -2.2545, -3.6737,\n",
      "         -2.0771, -1.6143]], device='cuda:5', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0125, 0.0208, 0.0399, 0.0084, 0.0216, 0.0579, 0.0146, 0.0258, 0.0577,\n",
      "         0.0174, 0.0550, 0.1104, 0.0111, 0.0458, 0.0950, 0.0248, 0.1113, 0.1660]],\n",
      "       device='cuda:5', grad_fn=<SigmoidBackward0>)\n",
      "[[0.07769370824098587, 0.09177270531654358, 0.1035546213388443, 0.191818505525589, 0.15865221619606018, 0.3181720972061157]]\n"
     ]
    }
   ],
   "source": [
    "text = ['*与n3：啊？']\n",
    "# 爱、乐、惊、怒、恐、哀\n",
    "print(get_output(device1,cls,text,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs0",
   "language": "python",
   "name": "bs0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
