{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list(text_path):\n",
    "    lsit=[]\n",
    "    with open('%s' % text_path, 'r', encoding=\"utf8\") as f:  # 打开一个文件只读模式\n",
    "        line = f.readlines()  # 读取文件中的每一行，放入line列表中\n",
    "        for line_list in line:\n",
    "            lsit.append(line_list.replace('\\n',''))\n",
    "    return lsit\n",
    "train_x=read_list('../data/pre_data.txt')\n",
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ban是cls的后缀\n",
    "ban='_1636532638.5109255_1_0.6603_0.7026'\n",
    "cuda_num = str(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "        self.model.to(device)\n",
    "        self.sig=nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, 18)\n",
    "        self.l2 = nn.Linear(18, 6)\n",
    "#         self.l2 = nn.Linear(768, 6)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "#         print(outputs[0])torch.Size([8, 100, 768])\n",
    "#         print(outputs[1])torch.Size([8, 768])\n",
    "#         print(outputs[0][:,0,:])torch.Size([8, 768])\n",
    "#         print(outputs[1])torch.Size([8, 768])\n",
    "        x = outputs[1]\n",
    "        x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.sig(x)\n",
    "        \n",
    "        x0=self.l2(x)\n",
    "        return x,x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device_s = \"cuda:\" + cuda_num\n",
    "device = torch.device(device_s if torch.cuda.is_available() else \"cpu\")\n",
    "cls=torch.load(\"../data/cls\"+str(ban)+\".model\",map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(text):\n",
    "    with torch.no_grad():\n",
    "        text2id = tokenizer(\n",
    "            text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids=text2id[\"input_ids\"].to(device)\n",
    "        mask=text2id[\"attention_mask\"].to(device)\n",
    "        output18,output6 = cls(input_ids, attention_mask=mask)\n",
    "        output=output6.round()\n",
    "        for i in output0:#18\n",
    "            s=''\n",
    "            for j in i:\n",
    "                s+=str(j)\n",
    "                s+=','\n",
    "            s=s[:-1]\n",
    "            output1.append(s)\n",
    "        \n",
    "    return output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['*与n3：啊？','*与n3：啊？']\n",
    "# 爱、乐、惊、怒、恐、哀\n",
    "print(get_output(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "\n",
    "i=0\n",
    "batch_size=16\n",
    "while(i<len(train_x)):\n",
    "    if i+batch_size<=len(train_x):\n",
    "        input_x=train_x[i:i+batch_size]\n",
    "        print('{}:{},len={}'.format(i,i+batch_size,batch_size),end = \"\\r\")\n",
    "    else:\n",
    "        input_x=train_x[i:len(train_x)]\n",
    "        print('{}:{},len={}'.format(i,len(train_x)-1,len(train_x)-i),end = \"\\r\")\n",
    "        \n",
    "    pre=get_output(input_x)\n",
    "#     print(pre)\n",
    "    result+=pre\n",
    "#     print(len(result))\n",
    "    i+=batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_list=read_list('../data/end_list.txt')\n",
    "print(end_list[0])\n",
    "print(len(result))\n",
    "print(len(end_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = open('../data/result'+str(ban)+'.txt', encoding='utf-8', mode='w')\n",
    "ff.write('id\\temotion\\n')\n",
    "for i in range(len(end_list)):\n",
    "    ff.write(end_list[i]+'\\t'+result[i])\n",
    "    ff.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs0",
   "language": "python",
   "name": "bs0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
