{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "穿着背心的*醒来，看看手机，三点了。\n"
     ]
    }
   ],
   "source": [
    "def read_list(text_path):\n",
    "    lsit=[]\n",
    "    with open('%s' % text_path, 'r', encoding=\"utf8\") as f:  # 打开一个文件只读模式\n",
    "        line = f.readlines()  # 读取文件中的每一行，放入line列表中\n",
    "        for line_list in line:\n",
    "            lsit.append(line_list.replace('\\n',''))\n",
    "    return lsit\n",
    "train_x=read_list('../data/pre_data.txt')\n",
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ban是cls的后缀\n",
    "ban='_1636180246.4086099_0_0.7081_0.7007'\n",
    "cuda_num = str(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "        self.model.to(device)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, 6)\n",
    "#         self.l2 = nn.Linear(768, 6)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "#         print(outputs[0])torch.Size([8, 100, 768])\n",
    "#         print(outputs[1])torch.Size([8, 768])\n",
    "#         print(outputs[0][:,0,:])torch.Size([8, 768])\n",
    "#         print(outputs[1])torch.Size([8, 768])\n",
    "        x = outputs[1]\n",
    "        x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device_s = \"cuda:\" + cuda_num\n",
    "device = torch.device(device_s if torch.cuda.is_available() else \"cpu\")\n",
    "cls=torch.load(\"../data/cls\"+str(ban)+\".model\",map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(text):\n",
    "    text2id = tokenizer(\n",
    "        text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids=text2id[\"input_ids\"].to(device)\n",
    "    mask=text2id[\"attention_mask\"].to(device)\n",
    "    output = cls(input_ids, attention_mask=mask)\n",
    "\n",
    "#     output=output.round()\n",
    "#     output=output.int()\n",
    "    output=output.tolist()\n",
    "    output0=[]\n",
    "    for j in output:\n",
    "        s=''\n",
    "        for i in j:\n",
    "            s+=str(i)\n",
    "            s+=','\n",
    "        s=s[:-1]\n",
    "        output0.append(s)\n",
    "    return output0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.005654674954712391,0.05292457714676857,0.42862987518310547,-0.0011745281517505646,0.021912939846515656,0.019521398469805717', '-0.005654674954712391,0.05292457714676857,0.42862987518310547,-0.0011745281517505646,0.021912939846515656,0.019521398469805717']\n"
     ]
    }
   ],
   "source": [
    "text = ['*与n3：啊？','*与n3：啊？']\n",
    "# 爱、乐、惊、怒、恐、哀\n",
    "print(get_output(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21376\r"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "\n",
    "i=0\n",
    "batch_size=16\n",
    "while(i<len(train_x)):\n",
    "    if i+batch_size<=len(train_x):\n",
    "        input_x=train_x[i:i+batch_size]\n",
    "#         print('{}:{},len={}'.format(i,i+batch_size,batch_size),end = \"\\r\")\n",
    "    else:\n",
    "        input_x=train_x[i:len(train_x)]\n",
    "#         print('{}:{},len={}'.format(i,len(train_x)-1,len(train_x)-i),end = \"\\r\")\n",
    "        \n",
    "    pre=get_output(input_x)\n",
    "#     print(pre)\n",
    "    result+=pre\n",
    "    print(len(result),end = \"\\r\")\n",
    "    i+=batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34170_0002_A_12\n",
      "21376\n",
      "21376\n"
     ]
    }
   ],
   "source": [
    "end_list=read_list('../data/end_list.txt')\n",
    "print(end_list[0])\n",
    "print(len(result))\n",
    "print(len(end_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = open('../data/result'+str(ban)+'.txt', encoding='utf-8', mode='w')\n",
    "ff.write('id\\temotion\\n')\n",
    "for i in range(len(end_list)):\n",
    "    ff.write(end_list[i]+'\\t'+result[i])\n",
    "    ff.write(\"\\n\")\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs0",
   "language": "python",
   "name": "bs0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
