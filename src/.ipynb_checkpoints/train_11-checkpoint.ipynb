{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ban=0\n",
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device0 = torch.device('cuda:6' if torch.cuda.is_available() else \"cpu\")\n",
    "device1 = torch.device('cuda:6' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读数据\n",
    "import pandas as pd\n",
    "df_train=pd.read_excel(\"../data/df_train.xlsx\",index_col=0)\n",
    "df_test=pd.read_excel(\"../data/df_test.xlsx\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30102/30102 [00:02<00:00, 12557.65it/s]\n",
      "100%|██████████| 6680/6680 [00:00<00:00, 11875.24it/s]\n"
     ]
    }
   ],
   "source": [
    "y=[]\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(df_train))):\n",
    "    if df_train.loc[i]['label']=='[0, 0, 0, 0, 0, 0]':\n",
    "        y.append(0)\n",
    "    else:\n",
    "#         print(df_train.loc[i]['label'])\n",
    "        y.append(1)\n",
    "df_train['label']=y\n",
    "y=[]\n",
    "for i in tqdm(range(len(df_test))):\n",
    "    if df_test.loc[i]['label']=='[0, 0, 0, 0, 0, 0]':\n",
    "        y.append(0)\n",
    "    else:\n",
    "#         print(df_test.loc[i]['label'])\n",
    "        y.append(1)\n",
    "df_test['label']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text=[df_train['text'][i].replace('*','##char##') for i in range(len(df_train))]\n",
    "df_test_text=[df_test['text'][i].replace('*','##char##') for i in range(len(df_test))]\n",
    "df_train['text']=df_train_text\n",
    "df_test['text']=df_test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "added_token=['##char##']\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\",additional_special_tokens=added_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2id_train = tokenizer(\n",
    "        df_train_text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "input_ids_train=text2id_train[\"input_ids\"]\n",
    "mask_train=text2id_train[\"attention_mask\"]\n",
    "\n",
    "text2id_test = tokenizer(\n",
    "        df_test_text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "input_ids_test=text2id_test[\"input_ids\"]\n",
    "mask_test=text2id_test[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['input_ids']=input_ids_train.tolist()\n",
    "df_train['mask']=mask_train.tolist()\n",
    "\n",
    "df_test['input_ids']=input_ids_test.tolist()\n",
    "df_test['mask']=mask_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.dataset = df\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"text\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "#         print(label)\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        mask = self.dataset.loc[idx, \"mask\"]\n",
    "        sample = {\"text\": text, \"label\": label,\"input_ids\":input_ids,\"mask\":mask}\n",
    "        # print(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fe3a21f7c50>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7fe3a21f7e50>\n"
     ]
    }
   ],
   "source": [
    "#按batch_size分\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    SentimentDataset(df_train), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "print(train_loader)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    SentimentDataset(df_test), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class fn2_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn2_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "        self.model.resize_token_embeddings(len(tokenizer))##############\n",
    "        self.model.to(device)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, 1)\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "        x = outputs[1]\n",
    "#         x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        return x\n",
    "cls = fn2_cls(device0)\n",
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # ban='_1637317433.7353098_1_0.6902_0.7111'\n",
    "\n",
    "# if ban==0:\n",
    "#     cls = fn2_cls(device0)\n",
    "# else:\n",
    "#     cls=torch.load(\"../data/cls\"+str(ban)+\".model\",map_location=device0)\n",
    "\n",
    "# sigmoid = nn.Sigmoid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(device_test):\n",
    "    cls.to(device_test)\n",
    "    cls.eval()\n",
    "\n",
    "    ii=0\n",
    "    epoch_loss=0\n",
    "    for batch_idx,batch in tqdm(enumerate(test_loader)):\n",
    "        with torch.no_grad():\n",
    "            label=batch['label'].to(device_test).float().view(-1,1)\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device_test)\n",
    "            mask=torch.stack(batch['mask']).t().to(device_test)\n",
    "\n",
    "            output = cls(input_ids, attention_mask=mask)\n",
    "            output=sigmoid(output)\n",
    "            #计算loss\n",
    "            loss = criterion(output, label)\n",
    "            epoch_loss+=loss\n",
    "            #计算输出\n",
    "            output=output.round()\n",
    "            if ii==0:\n",
    "                all_output=output.view(1,-1)\n",
    "                all_label=label.view(1,-1)\n",
    "                ii=1\n",
    "            else:\n",
    "#                 print(all_output,output)\n",
    "                all_output=torch.cat((all_output,output.view(1,-1)),1)\n",
    "                all_label=torch.cat((all_label,label.view(1,-1)),1)\n",
    "        \n",
    "    all_output=np.array(all_output.t().cpu())\n",
    "    all_label=np.array(all_label.t().cpu())\n",
    "#     can't convert cuda:5 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n",
    "    acc_score=metrics.accuracy_score(all_output, all_label)\n",
    "    print(metrics.classification_report(all_output, all_label))\n",
    "    print(\"准确率:\",acc_score )\n",
    "    \n",
    "    return acc_score,epoch_loss.item()\n",
    "\n",
    "# test(device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # 引入time模块\n",
    "\n",
    "import torch\n",
    "\n",
    "def train(device_train,device_test,epoch_num,epoch_be=0):\n",
    "    for epoch_idx in range(epoch_be,epoch_be+epoch_num):\n",
    "        epoch_loss=0\n",
    "        cls.to(device_train)\n",
    "        cls.train()\n",
    "        ii=0\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________ epoch:\"+str(epoch_idx)+\" start _________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        for batch_idx,batch in enumerate(train_loader):\n",
    "            print(batch_idx,len(train_loader),batch_idx/len(train_loader),'epoch_loss:',epoch_loss,end='\\r')\n",
    "            \n",
    "            label=batch['label'].to(device_train).float().view(-1,1)\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device_train)\n",
    "            mask=torch.stack(batch['mask']).t().to(device_train)\n",
    "\n",
    "            output = cls(input_ids, attention_mask=mask)\n",
    "            output=sigmoid(output)\n",
    "#             print(output)\n",
    "            \n",
    "            # 梯度积累\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #计算score\n",
    "            with torch.no_grad():\n",
    "                epoch_loss+=loss\n",
    "#                 print(output.tolist())\n",
    "                output=output.round()\n",
    "#                 print(output.tolist())\n",
    "#                 print(label.tolist())\n",
    "                if ii==0:\n",
    "                    all_output=output.view(1,-1)\n",
    "                    all_label=label.view(1,-1)\n",
    "                    ii=1\n",
    "                else:\n",
    "#                 \n",
    "                    all_output=torch.cat((all_output,output.view(1,-1)),1)\n",
    "                    all_label=torch.cat((all_label,label.view(1,-1)),1)\n",
    "#                 print(all_output,all_label)\n",
    "\n",
    "        #每个epoch结束：\n",
    "        with torch.no_grad():\n",
    "            all_output=np.array(all_output.t().cpu())\n",
    "            all_label=np.array(all_label.t().cpu())\n",
    "        #     can't convert cuda:5 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n",
    "            acc_score=metrics.accuracy_score(all_output, all_label)\n",
    "            print(metrics.classification_report(all_output, all_label))\n",
    "            print(\"准确率:\",acc_score)\n",
    "        \n",
    "        acc_score_test,epoch_loss_test=test(device_test)\n",
    "        \n",
    "        print('acc_score:',acc_score,'epoch_loss:',epoch_loss,'acc_score_test:',acc_score_test,'epoch_loss_test:',epoch_loss_test)\n",
    "        #保存模型\n",
    "        end = time.time()\n",
    "        print('epoch:',str(epoch_idx)+'_执行时间: ',end - start)\n",
    "#         torch.save(cls,\"../data/cls_\"+str(end)+\"_\"+str(epoch_idx)+\"_\"+str(round(acc_score,4))+\"_\"+str(round(acc_score_test,4))+\".model\")\n",
    "\n",
    "\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________ epoch:\"+str(epoch_idx)+\" end _________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "418it [00:15, 27.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.06      0.53      0.10       309\n",
      "         1.0       0.96      0.56      0.71      6371\n",
      "\n",
      "    accuracy                           0.56      6680\n",
      "   macro avg       0.51      0.55      0.40      6680\n",
      "weighted avg       0.92      0.56      0.68      6680\n",
      "\n",
      "准确率: 0.5588323353293413\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:0 start _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "              precision    recall  f1-score   support2295, device='cuda:6')\n",
      "\n",
      "         0.0       0.80      0.71      0.75     19805\n",
      "         1.0       0.55      0.66      0.60     10297\n",
      "\n",
      "    accuracy                           0.70     30102\n",
      "   macro avg       0.67      0.69      0.68     30102\n",
      "weighted avg       0.72      0.70      0.70     30102\n",
      "\n",
      "准确率: 0.6958009434589064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "418it [00:15, 26.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.56      0.66      4378\n",
      "         1.0       0.48      0.77      0.59      2302\n",
      "\n",
      "    accuracy                           0.63      6680\n",
      "   macro avg       0.65      0.66      0.63      6680\n",
      "weighted avg       0.70      0.63      0.64      6680\n",
      "\n",
      "准确率: 0.6287425149700598\n",
      "acc_score: 0.6958009434589064 epoch_loss: tensor(1082.6835, device='cuda:6') acc_score_test: 0.6287425149700598 epoch_loss_test: 283.1186828613281\n",
      "epoch: 0_执行时间:  232.6150825023651\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:0 end _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:1 start _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "881 1882 0.4681190223166844 epoch_loss: tensor(424.8867, device='cuda:6'))\r"
     ]
    }
   ],
   "source": [
    "# zero=0\n",
    "# one=0\n",
    "# for i in range(len(df_train)): \n",
    "#     if df_train.loc[i]['label']==1:\n",
    "#         one+=1\n",
    "#     else:\n",
    "#         zero+=1\n",
    "    \n",
    "# weight = torch.tensor([one/(one+zero)]).to(device1)\n",
    "# print(weight)\n",
    "# criterion = nn.BCELoss(weight=weight)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "cls = fn2_cls(device0)\n",
    "from torch import optim\n",
    "optimizer = optim.Adam(cls.parameters(), lr=1e-5)\n",
    "test(device1)\n",
    "train(device0,device1,epoch_num=3,epoch_be=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(device0,device1,epoch_num=3,epoch_be=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# end = time.time()\n",
    "# torch.save(cls,\"../data/cls_\"+str(end)+\"_\"+str(0)+\"_\"+str(0.6771)+\"_\"+str(0.7125)+\".model\")\n",
    "\n",
    "# torch.save(cls,\"../data/cls\"+str(ban+50)+\".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "def get_output(text,device):\n",
    "        cls.to(device)\n",
    "        cls.eval\n",
    "        text2id = tokenizer(\n",
    "            text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids=text2id[\"input_ids\"].to(device)\n",
    "        mask=text2id[\"attention_mask\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = cls(input_ids, attention_mask=mask)\n",
    "            output1=sigmoid(output)\n",
    "        return output1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5693],\n",
      "        [0.6059],\n",
      "        [0.5247],\n",
      "        [0.5777],\n",
      "        [0.5239],\n",
      "        [0.4713]], device='cuda:5')\n"
     ]
    }
   ],
   "source": [
    "text = ['##char##与n3：啊？',\n",
    "        '##char##一手拿着一个行李，一路小跑着把c1带到了文工团门口',\n",
    "        '##char##开心地点了点头。',\n",
    "        '##char##笑了笑：军礼不是这么敬的。五指并拢，大臂带动小臂，举到齐眉。'\n",
    "       '##char##看到o2拎着行李进来，往他那边走：哟，o2回来了，欢迎我们全军的学雷锋标兵凯旋归来啊！',\n",
    "        'd1看到##char##拎着行李进来，往他那边走：哟，##char##回来了，欢迎我们全军的学雷锋标兵凯旋归来啊！',\n",
    "        \n",
    "       ]\n",
    "# 爱、乐、惊、怒、恐、哀\n",
    "print(get_output(text,device1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs0",
   "language": "python",
   "name": "bs0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
