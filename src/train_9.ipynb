{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ban=0\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device0 = torch.device('cuda:5' if torch.cuda.is_available() else \"cpu\")\n",
    "device1 = torch.device('cuda:5' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读数据\n",
    "import pandas as pd\n",
    "df_train=pd.read_excel(\"../data/df_train.xlsx\",index_col=0)\n",
    "df_test=pd.read_excel(\"../data/df_test.xlsx\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "label18=[]\n",
    "for i in range(len(df_train)):\n",
    "    new_list=[]\n",
    "    old_list=ast.literal_eval(df_train.loc[i]['label'])\n",
    "    df_train.loc[i]['label']=old_list\n",
    "    for j in old_list:\n",
    "        if j==0:\n",
    "            new_list+=[0,0,0]\n",
    "        if j==1:\n",
    "            new_list+=[0,0,1]\n",
    "        if j==2:\n",
    "            new_list+=[0,1,1]\n",
    "        if j==3:\n",
    "            new_list+=[1,1,1]\n",
    "    label18.append(new_list)\n",
    "\n",
    "df_train['label18']=label18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label18=[]\n",
    "for i in range(len(df_test)):\n",
    "    new_list=[]\n",
    "    old_list=ast.literal_eval(df_test.loc[i]['label'])\n",
    "    df_test.loc[i]['label']=old_list\n",
    "    for j in old_list:\n",
    "        if j==0:\n",
    "            new_list+=[0,0,0]\n",
    "        if j==1:\n",
    "            new_list+=[0,0,1]\n",
    "        if j==2:\n",
    "            new_list+=[0,1,1]\n",
    "        if j==3:\n",
    "            new_list+=[1,1,1]\n",
    "    label18.append(new_list)\n",
    "\n",
    "df_test['label18']=label18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "df_train_text=[df_train['text'][i] for i in range(len(df_train))]\n",
    "df_test_text=[df_test['text'][i] for i in range(len(df_test))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2id_train = tokenizer(\n",
    "        df_train_text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "input_ids_train=text2id_train[\"input_ids\"]\n",
    "mask_train=text2id_train[\"attention_mask\"]\n",
    "\n",
    "text2id_test = tokenizer(\n",
    "        df_test_text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "input_ids_test=text2id_test[\"input_ids\"]\n",
    "mask_test=text2id_test[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['input_ids']=input_ids_train.tolist()\n",
    "df_train['mask']=mask_train.tolist()\n",
    "\n",
    "df_test['input_ids']=input_ids_test.tolist()\n",
    "df_test['mask']=mask_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_att(input_ids_train_):\n",
    "    index=0\n",
    "    _115=[]\n",
    "    for i in input_ids_train_:\n",
    "        if i==115:\n",
    "#             print(i)\n",
    "            _115.append(index)\n",
    "        index+=1\n",
    "#     print('_115:',_115)\n",
    "    ###########################################################\n",
    "                \n",
    "    if len(_115)==0:\n",
    "        att=[]\n",
    "        for i in input_ids_train_:\n",
    "            if i in [101,102,0]:\n",
    "                att.append(0)\n",
    "            else:\n",
    "                att.append(1)\n",
    "        return att\n",
    "    \n",
    "    \n",
    "    ###########################################################\n",
    "    \n",
    "    \n",
    "    att=[]\n",
    "\n",
    "    index=0\n",
    "    max_=0\n",
    "    for i in input_ids_train_:\n",
    "        if i in [101,102,0]:\n",
    "            att.append(-1)\n",
    "            index+=1\n",
    "            continue\n",
    "        att_t=[]\n",
    "        for j in _115:\n",
    "            att_t.append(abs(index-j))\n",
    "\n",
    "        att.append(min(att_t))\n",
    "        if min(att_t)>max_:\n",
    "            max_=min(att_t)\n",
    "        index+=1\n",
    "        \n",
    "#     print('att:',att)\n",
    "    \n",
    "    ###########################################################\n",
    "    att0=[]\n",
    "    sum_=0\n",
    "    num_=0\n",
    "    for j in att:\n",
    "        if j==-1:\n",
    "            att0.append(0)\n",
    "        else:\n",
    "            att0.append(max_-j)\n",
    "            sum_+=max_-j\n",
    "            num_+=1\n",
    "    att1=[]\n",
    "    sum_=sum_/num_\n",
    "    for j in att0:\n",
    "        att1.append(j/sum_)\n",
    "        \n",
    "#     print('att1:',att1)\n",
    "    return att1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train_att=[get_att(i) for i in input_ids_train]\n",
    "input_ids_test_att=[get_att(i) for i in input_ids_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['att']=input_ids_train_att\n",
    "df_test['att']=input_ids_test_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.dataset = df\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"text\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "#         print(label)\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        mask = self.dataset.loc[idx, \"mask\"]\n",
    "        label18=self.dataset.loc[idx, \"label18\"]\n",
    "        att=self.dataset.loc[idx, \"att\"]\n",
    "        sample = {\"text\": text, \"label\": label,\"label18\": label18,\"input_ids\":input_ids,\"mask\":mask,'att':att}\n",
    "        # print(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fed71662610>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7fed71662110>\n"
     ]
    }
   ],
   "source": [
    "#按batch_size分\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    SentimentDataset(df_train), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "print(train_loader)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    SentimentDataset(df_test), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "        self.model.to(device)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, 18)\n",
    "\n",
    "    def forward(self, x, att, attention_mask=None):\n",
    "        #att:batch size*100\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "        x = outputs[0]#batch size*100*768\n",
    "        \n",
    "        \n",
    "        #100*768    and   100 ---->>>>   768\n",
    "        all_e=[]\n",
    "        for i in range(len(att)):\n",
    "            bb=att[i]#100\n",
    "            aa=x[i]#100*768\n",
    "            cc=aa*bb.view(100,1)\n",
    "            all_e.append(torch.sum(cc,0))#1*768\n",
    "            \n",
    "        x=torch.stack(all_e)#batch size*768\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if ban==0:\n",
    "    cls = fn_cls(device0)\n",
    "    cls.train()\n",
    "else:\n",
    "    cls=torch.load(\"../data/cls\"+str(ban)+\".model\",map_location=device0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _18to6_(output,round_=0):#18tensor 变 6list\n",
    "    output0=[]\n",
    "    for j in output:#18\n",
    "        i=0\n",
    "        s=''\n",
    "        list6=[]\n",
    "        while(i<len(j)):\n",
    "            ok=j[i]*1.2+j[i+1]*1.1+j[i+2]*1\n",
    "            if round_==1:\n",
    "                list6.append(ok.round().int().tolist())\n",
    "            else:\n",
    "                list6.append(ok.tolist())\n",
    "            i+=3\n",
    "        output0.append(list6)\n",
    "    return output0\n",
    "\n",
    "def get_loss_test(output,A,round_=0):#8*18\n",
    "    output_r=torch.Tensor(_18to6_(output,round_=1)).to(device0)\n",
    "    output=_18to6_(output,round_=round_)\n",
    "    # print(output_r,A)\n",
    "    cor_add=(output_r == A).sum().item()\n",
    "    sum0=0\n",
    "    for i in range(len(output)):\n",
    "        for j in range(6):\n",
    "            sum0+=(output[i][j]-A[i][j])*(output[i][j]-A[i][j])\n",
    "    return sum0,cor_add\n",
    "\n",
    "def test(device_test):\n",
    "    cls.to(device_test)\n",
    "    cls.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_test=0\n",
    "    for batch_idx,batch in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "#             print(batch['att'])\n",
    "            # print(batch['label'],batch['label18'])\n",
    "            label6=torch.stack(batch['label']).t().to(device_test).float()\n",
    "            # label18=torch.stack(batch['label18']).t().to(device_test).float()\n",
    "\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device_test)\n",
    "            mask=torch.stack(batch['mask']).t().to(device_test)\n",
    "            att=torch.stack(batch['att']).t().to(device_test).float()\n",
    "            \n",
    "            output = cls(input_ids, att,attention_mask=mask)\n",
    "            output=sigmoid(output)\n",
    "\n",
    "            total += len(output)*6\n",
    "            loss_add,cor_add = get_loss_test(output, label6 ,round_=1)\n",
    "            loss_test+=loss_add\n",
    "            correct+=cor_add\n",
    "\n",
    "            tes_score=1/(1+(loss_test/total) ** 0.5)\n",
    "            acc_score=100.*correct/total\n",
    "\n",
    "        \n",
    "\n",
    "        print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{:.2f}%，score：{}'.format(\n",
    "                    batch_idx, len(test_loader),100.*batch_idx/len(test_loader), \n",
    "                    correct, total,acc_score,\n",
    "                    tes_score\n",
    "            ),end='\\r')\n",
    "    print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{:.2f}%，score：{}'.format(\n",
    "                    batch_idx, len(test_loader),100.*batch_idx/len(test_loader), \n",
    "                    correct, total,acc_score,\n",
    "                    tes_score\n",
    "            ))\n",
    "#     cls.to(device_train)\n",
    "    return tes_score,acc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test(device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24585.0\n",
      "[408.0, 722.0, 1132.0, 221.0, 646.0, 1868.0, 280.0, 752.0, 1566.0, 544.0, 1589.0, 2994.0, 378.0, 1202.0, 2130.0, 932.0, 2638.0, 4583.0]\n"
     ]
    }
   ],
   "source": [
    "weight=[]\n",
    "import numpy as np\n",
    "sumn=np.zeros(18)\n",
    "for i in range(len(df_train)):\n",
    "    print(i,end='\\r')\n",
    "    sumn+=np.array(df_train.loc[i]['label18'])\n",
    "#     print(sumn,end='\\r')\n",
    "\n",
    "print(sumn.sum())\n",
    "print(sumn.tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.232843137254902, 6.347645429362881, 4.048586572438163, 20.737556561085974, 7.094427244582043, 2.4534261241970023, 16.367857142857144, 6.094414893617022, 2.926564495530013, 8.424632352941176, 2.8842039018250474, 1.5307281229124916, 12.124338624338625, 3.812811980033278, 2.151643192488263, 4.917381974248927, 1.7373009855951478, 1.0]\n",
      "tensor([11.2328,  6.3476,  4.0486, 20.7376,  7.0944,  2.4534, 16.3679,  6.0944,\n",
      "         2.9266,  8.4246,  2.8842,  1.5307, 12.1243,  3.8128,  2.1516,  4.9174,\n",
      "         1.7373,  1.0000], device='cuda:5')\n"
     ]
    }
   ],
   "source": [
    "sumn=sumn.tolist()\n",
    "weight=[max(sumn)/i for i in sumn]\n",
    "print(weight)\n",
    "weight=torch.Tensor(weight).to(device0)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "#BCEWithLogitsLoss就是把Sigmoid-BCELoss合成一步\n",
    "criterion = nn.BCELoss(weight=weight)\n",
    "criterion_hui=nn.MSELoss()\n",
    "optimizer = optim.Adam(cls.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:0 start _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "labels: tensor([[0., 0., 0., 1., 0., 0.],.636596\t准确率：83.25%\n",
      "        [0., 0., 0., 0., 0., 0.]], device='cuda:5')\n",
      "pred: tensor([[8.5158e-02, 1.4223e-02, 3.9106e-02, 1.5378e-01, 1.5377e-02, 1.2957e-01],\n",
      "        [1.1675e-02, 6.7749e-05, 1.4100e-01, 3.9017e-03, 4.6558e-01, 3.3947e-02]],\n",
      "       device='cuda:5')\n",
      "Train Epoch: 0\tscore:0.636609\t准确率：83.25%\n",
      "[1669/1670 (100%)]\t正确分类的样本数：35495，样本总数：40080，准确率：88.56%，score：0.6941369771957397\n",
      "epoch: 0_执行时间:  419.872460603714\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:0 end _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:1 start _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "labels: tensor([[0., 0., 0., 0., 0., 0.],.682104\t准确率：89.16%\n",
      "        [0., 0., 0., 0., 0., 2.]], device='cuda:5')\n",
      "pred: tensor([[6.9564e-01, 5.8316e-01, 1.6385e-05, 3.2056e-03, 2.4235e-03, 1.1815e-02],\n",
      "        [1.6276e-02, 3.4276e-03, 5.8410e-02, 6.3329e-03, 1.2167e-02, 3.4406e-01]],\n",
      "       device='cuda:5')\n",
      "Train Epoch: 1\tscore:0.682095\t准确率：89.16%\n",
      "[1669/1670 (100%)]\t正确分类的样本数：34407，样本总数：40080，准确率：85.85%，score：0.6878635883331299\n",
      "epoch: 1_执行时间:  412.41258549690247\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:1 end _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:2 start _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "labels: tensor([[0., 0., 0., 0., 0., 0.],.711278\t准确率：90.56%\n",
      "        [2., 0., 0., 0., 0., 2.]], device='cuda:5')\n",
      "pred: tensor([[3.1605e-02, 3.6838e-02, 4.6151e-02, 4.7649e-02, 2.0294e-01, 2.4152e-01],\n",
      "        [2.7637e-01, 8.9779e-04, 1.3926e-04, 3.8056e-03, 9.7022e-02, 4.0955e-01]],\n",
      "       device='cuda:5')\n",
      "Train Epoch: 2\tscore:0.711274\t准确率：90.55%\n",
      "[1669/1670 (100%)]\t正确分类的样本数：35103，样本总数：40080，准确率：87.58%，score：0.6916908025741577\n",
      "epoch: 2_执行时间:  412.93849420547485\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:2 end _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:3 start _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "labels: tensor([[0., 0., 0., 0., 0., 2.],.739877\t准确率：92.06%\n",
      "        [0., 0., 0., 0., 0., 2.]], device='cuda:5')\n",
      "pred: tensor([[3.1095e-01, 2.5376e-03, 1.6798e-03, 2.3113e-02, 1.1512e-01, 1.0494e+00],\n",
      "        [9.1823e-03, 1.1451e-02, 4.7027e-04, 3.1572e-05, 1.3605e-03, 9.4133e-01]],\n",
      "       device='cuda:5')\n",
      "Train Epoch: 3\tscore:0.739873\t准确率：92.06%\n",
      "[1669/1670 (100%)]\t正确分类的样本数：34989，样本总数：40080，准确率：87.30%，score：0.6919454336166382\n",
      "epoch: 3_执行时间:  410.8105137348175\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:3 end _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import time  # 引入time模块\n",
    " \n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(device_train,device_test,epoch_be=0):\n",
    "    epoch_num=4\n",
    "    accumulation_steps=32\n",
    "    \n",
    "    train_score=[]\n",
    "    test_score=[]\n",
    "    test_acc=[]\n",
    "\n",
    "    for epoch_idx in range(epoch_be,epoch_be+epoch_num):\n",
    "        cls.to(device_train)\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________ epoch:\"+str(epoch_idx)+\" start _________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        score_list=[]\n",
    "        loss_a=0\n",
    "        total=0\n",
    "        correct=0\n",
    "        start = time.time()\n",
    "        for batch_idx,batch in enumerate(train_loader):\n",
    "            label6=torch.stack(batch['label']).t().to(device_train).float()\n",
    "            label18=torch.stack(batch['label18']).t().to(device_train).float()\n",
    "\n",
    "            #计算output\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device_train)\n",
    "            mask=torch.stack(batch['mask']).t().to(device_train)\n",
    "            att=torch.stack(batch['att']).t().to(device_test).float()\n",
    "            \n",
    "            output = cls(input_ids, att,attention_mask=mask)\n",
    "            output=sigmoid(output)\n",
    "            output6=torch.Tensor(_18to6_(output,round_=0)).to(device_train)\n",
    "\n",
    "\n",
    "            # 梯度积累\n",
    "            loss0 = criterion(output, label18)\n",
    "            lossh=criterion_hui(output6,label6)\n",
    "\n",
    "            loss=loss0+lossh\n",
    "            loss = loss/accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            #计算score\n",
    "            with torch.no_grad():\n",
    "                total += len(output)*6\n",
    "                loss_add,cor_add = get_loss_test(output, label6,round_=0)\n",
    "                loss_a+=loss_add\n",
    "                correct+=cor_add\n",
    "\n",
    "                tr_score=1/(1+(loss_a/total) ** 0.5)\n",
    "                acc_score=100.*correct/total\n",
    "\n",
    "\n",
    "            if((batch_idx+1) % accumulation_steps) == 0:\n",
    "                # 每 accumulation_steps 次更新一下网络中的参数\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if ((batch_idx+1) % accumulation_steps) == 1:\n",
    "                score_list.append(tr_score)\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tscore:{:.6f}\\t准确率：{:.2f}%'.format(\n",
    "                    epoch_idx, batch_idx, len(train_loader), \n",
    "                    100.*batch_idx/len(train_loader), tr_score,acc_score\n",
    "                ),end='\\r')\n",
    "\n",
    "        #每个epoch结束：\n",
    "        print('labels:', label6)\n",
    "        print('pred:', output6)\n",
    "#         plt.plot([i for i in range(len(score_list))], score_list)\n",
    "#         plt.show()\n",
    "\n",
    "        print('Train Epoch: {}\\tscore:{:.6f}\\t准确率：{:.2f}%'.format(epoch_idx,tr_score,acc_score))\n",
    "        t_score,t_acc=test(device_test)\n",
    "\n",
    "        train_score.append(tr_score)\n",
    "        test_score.append(t_score)\n",
    "        test_acc.append(t_acc)\n",
    "\n",
    "        #保存模型\n",
    "        end = time.time()\n",
    "        print('epoch:',str(epoch_idx)+'_执行时间: ',end - start)\n",
    "        torch.save(cls,\"../data/cls_\"+str(end)+\"_\"+str(epoch_idx)+\"_\"+str(round(tr_score.tolist(),4))+\"_\"+str(round(t_score.tolist(),4))+\".model\")\n",
    "\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________ epoch:\"+str(epoch_idx)+\" end _________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "\n",
    "#     plt.plot([i for i in range(len(train_score))], train_score)\n",
    "#     plt.show()             \n",
    "\n",
    "#     plt.plot([i for i in range(len(test_score))], test_score)\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.plot([i for i in range(len(test_acc))], test_acc)\n",
    "#     plt.show()\n",
    "    \n",
    "train(device0,device1,epoch_be=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "torch.save(cls,\"../data/cls_\"+str(end)+\"_\"+str(2)+\"_\"+str(0.6886)+\"_\"+str(0.6942)+\".model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "def get_output(device_pre,model,text,f=0):\n",
    "    model.to(device_pre)\n",
    "    model.eval()\n",
    "    text2id = tokenizer(\n",
    "        text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "#     print(text,text2id)\n",
    "    input_ids=text2id[\"input_ids\"].to(device_pre)\n",
    "    mask=text2id[\"attention_mask\"].to(device_pre)\n",
    "    att=torch.tensor([get_att(input_ids[0])]).to(device_pre)\n",
    "#         print(text2id)\n",
    "    output = model(input_ids,att, attention_mask=mask)\n",
    "    \n",
    "    print(output)\n",
    "    output=sigmoid(output)\n",
    "    print(output)\n",
    "    \n",
    "#     output=sigmoid(output)*4\n",
    "    if f==1:\n",
    "        return _18to6_(output,round_=1)\n",
    "    else:\n",
    "        return _18to6_(output,round_=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.1787, -4.2691, -2.9211, -6.9531, -4.3885, -4.4666, -2.7147, -1.6942,\n",
      "         -0.8299, -4.5495, -4.8081, -4.4052, -5.5299, -3.8132, -4.2456, -5.6641,\n",
      "         -5.1396, -4.1083]], device='cuda:5', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0056, 0.0138, 0.0511, 0.0010, 0.0123, 0.0114, 0.0621, 0.1552, 0.3037,\n",
      "         0.0105, 0.0081, 0.0121, 0.0040, 0.0216, 0.0141, 0.0035, 0.0058, 0.0162]],\n",
      "       device='cuda:5', grad_fn=<SigmoidBackward0>)\n",
      "[[0, 0, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "text = ['*与n3：啊？']\n",
    "# 爱、乐、惊、怒、恐、哀\n",
    "print(get_output(device1,cls,text,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs0",
   "language": "python",
   "name": "bs0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
