{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ban='_1636619329.723694_1_0.6912_0.7004'\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device0 = torch.device('cuda:5' if torch.cuda.is_available() else \"cpu\")\n",
    "device1 = torch.device('cuda:5' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读数据\n",
    "import pandas as pd\n",
    "df_train=pd.read_excel(\"../data/df_train.xlsx\",index_col=0)\n",
    "df_test=pd.read_excel(\"../data/df_test.xlsx\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "label18=[]\n",
    "for i in range(len(df_train)):\n",
    "    new_list=[]\n",
    "    old_list=ast.literal_eval(df_train.loc[i]['label'])\n",
    "    df_train.loc[i]['label']=old_list\n",
    "    for j in old_list:\n",
    "        if j==0:\n",
    "            new_list+=[0,0,0]\n",
    "        if j==1:\n",
    "            new_list+=[0,0,1]\n",
    "        if j==2:\n",
    "            new_list+=[0,1,1]\n",
    "        if j==3:\n",
    "            new_list+=[1,1,1]\n",
    "    label18.append(new_list)\n",
    "\n",
    "df_train['label18']=label18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label18=[]\n",
    "for i in range(len(df_test)):\n",
    "    new_list=[]\n",
    "    old_list=ast.literal_eval(df_test.loc[i]['label'])\n",
    "    df_test.loc[i]['label']=old_list\n",
    "    for j in old_list:\n",
    "        if j==0:\n",
    "            new_list+=[0,0,0]\n",
    "        if j==1:\n",
    "            new_list+=[0,0,1]\n",
    "        if j==2:\n",
    "            new_list+=[0,1,1]\n",
    "        if j==3:\n",
    "            new_list+=[1,1,1]\n",
    "    label18.append(new_list)\n",
    "\n",
    "df_test['label18']=label18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "df_train_text=[df_train['text'][i] for i in range(len(df_train))]\n",
    "df_test_text=[df_test['text'][i] for i in range(len(df_test))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2id_train = tokenizer(\n",
    "        df_train_text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "input_ids_train=text2id_train[\"input_ids\"]\n",
    "mask_train=text2id_train[\"attention_mask\"]\n",
    "\n",
    "text2id_test = tokenizer(\n",
    "        df_test_text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "input_ids_test=text2id_test[\"input_ids\"]\n",
    "mask_test=text2id_test[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['input_ids']=input_ids_train.tolist()\n",
    "df_train['mask']=mask_train.tolist()\n",
    "\n",
    "df_test['input_ids']=input_ids_test.tolist()\n",
    "df_test['mask']=mask_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.dataset = df\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"text\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "#         print(label)\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        mask = self.dataset.loc[idx, \"mask\"]\n",
    "        label18=self.dataset.loc[idx, \"label18\"]\n",
    "        sample = {\"text\": text, \"label\": label,\"label18\": label18,\"input_ids\":input_ids,\"mask\":mask}\n",
    "        # print(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f341ee7f090>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f341ee7f050>\n"
     ]
    }
   ],
   "source": [
    "#按batch_size分\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    SentimentDataset(df_train), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "print(train_loader)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    SentimentDataset(df_test), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "        self.model.to(device)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, 18)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "        x = outputs[1]\n",
    "        x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if ban==0:\n",
    "    cls = fn_cls(device0)\n",
    "    cls.train()\n",
    "else:\n",
    "    cls=torch.load(\"../data/cls\"+str(ban)+\".model\",map_location=device0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1669/1670 (100%)]\t正确分类的样本数：35199，样本总数：40080，准确率：87.82%，score：0.7145693898200989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.7146, device='cuda:5'), 87.82185628742515)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _18to6_(output,round_=0):#18tensor 变 6list\n",
    "    output0=[]\n",
    "    for j in output:#18\n",
    "        i=0\n",
    "        s=''\n",
    "        list6=[]\n",
    "        while(i<len(j)):\n",
    "            ok=j[i]*1.2+j[i+1]*1.1+j[i+2]*1\n",
    "            if round_==1:\n",
    "                list6.append(ok.round().int().tolist())\n",
    "            else:\n",
    "                list6.append(ok.tolist())\n",
    "            i+=3\n",
    "        output0.append(list6)\n",
    "    return output0\n",
    "\n",
    "def get_loss_test(output,A,round_=0):#8*18\n",
    "    output_r=torch.Tensor(_18to6_(output,round_=1)).to(device0)\n",
    "    output=_18to6_(output,round_=round_)\n",
    "    # print(output_r,A)\n",
    "    cor_add=(output_r == A).sum().item()\n",
    "    sum0=0\n",
    "    for i in range(len(output)):\n",
    "        for j in range(6):\n",
    "            sum0+=(output[i][j]-A[i][j])*(output[i][j]-A[i][j])\n",
    "    return sum0,cor_add\n",
    "\n",
    "def test(device_test):\n",
    "    cls.to(device_test)\n",
    "    cls.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_test=0\n",
    "    for batch_idx,batch in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            # print(batch['label'],batch['label18'])\n",
    "            label6=torch.stack(batch['label']).t().to(device_test).float()\n",
    "            # label18=torch.stack(batch['label18']).t().to(device_test).float()\n",
    "\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device_test)\n",
    "            mask=torch.stack(batch['mask']).t().to(device_test)\n",
    "\n",
    "            output = cls(input_ids, attention_mask=mask)\n",
    "            output=sigmoid(output)\n",
    "\n",
    "            total += len(output)*6\n",
    "            loss_add,cor_add = get_loss_test(output, label6 ,round_=0)\n",
    "            loss_test+=loss_add\n",
    "            correct+=cor_add\n",
    "\n",
    "            tes_score=1/(1+(loss_test/total) ** 0.5)\n",
    "            acc_score=100.*correct/total\n",
    "\n",
    "        \n",
    "\n",
    "        print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{:.2f}%，score：{}'.format(\n",
    "                    batch_idx, len(test_loader),100.*batch_idx/len(test_loader), \n",
    "                    correct, total,acc_score,\n",
    "                    tes_score\n",
    "            ),end='\\r')\n",
    "    print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{:.2f}%，score：{}'.format(\n",
    "                    batch_idx, len(test_loader),100.*batch_idx/len(test_loader), \n",
    "                    correct, total,acc_score,\n",
    "                    tes_score\n",
    "            ))\n",
    "#     cls.to(device_train)\n",
    "    return tes_score,acc_score\n",
    "\n",
    "test(device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24585.0\n",
      "[408.0, 722.0, 1132.0, 221.0, 646.0, 1868.0, 280.0, 752.0, 1566.0, 544.0, 1589.0, 2994.0, 378.0, 1202.0, 2130.0, 932.0, 2638.0, 4583.0]\n"
     ]
    }
   ],
   "source": [
    "weight=[]\n",
    "import numpy as np\n",
    "sumn=np.zeros(18)\n",
    "for i in range(len(df_train)):\n",
    "    print(i,end='\\r')\n",
    "    sumn+=np.array(df_train.loc[i]['label18'])\n",
    "#     print(sumn,end='\\r')\n",
    "\n",
    "print(sumn.sum())\n",
    "print(sumn.tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01659548505186089, 0.029367500508440107, 0.046044335977221885, 0.008989221069757983, 0.02627618466544641, 0.0759812894041082, 0.01138905836892414, 0.030587756762253407, 0.06369737644905431, 0.022127313402481186, 0.0646329062436445, 0.12178157413056742, 0.01537522879804759, 0.048891600569452916, 0.08663819402074435, 0.03790929428513321, 0.10730119991864959, 0.1864144803742119]\n",
      "tensor([0.0166, 0.0294, 0.0460, 0.0090, 0.0263, 0.0760, 0.0114, 0.0306, 0.0637,\n",
      "        0.0221, 0.0646, 0.1218, 0.0154, 0.0489, 0.0866, 0.0379, 0.1073, 0.1864],\n",
      "       device='cuda:5')\n"
     ]
    }
   ],
   "source": [
    "weight=[i/sumn.sum() for i in sumn.tolist()]\n",
    "print(weight)\n",
    "weight=torch.Tensor(weight).to(device0)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "#BCEWithLogitsLoss就是把Sigmoid-BCELoss合成一步\n",
    "criterion = nn.BCELoss(weight=weight)\n",
    "criterion_hui=nn.MSELoss()\n",
    "optimizer = optim.Adam(cls.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:0 start _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "labels: tensor([[0., 0., 0., 0., 0., 0.],.642422\t准确率：84.55%\n",
      "        [0., 0., 0., 0., 0., 0.]], device='cuda:5')\n",
      "pred: tensor([[0.0374, 0.0875, 0.0270, 0.1245, 0.0690, 0.2045],\n",
      "        [0.0415, 0.0382, 0.0439, 0.1128, 0.0760, 0.0953]], device='cuda:5')\n",
      "Train Epoch: 0\tscore:0.642436\t准确率：84.55%\n",
      "labels: tensor([[0., 0., 0., 0., 0., 0.],.691196\t准确率：90.34%.7422773241996765\n",
      "        [0., 0., 0., 2., 0., 0.]], device='cuda:5')\n",
      "pred: tensor([[0.0453, 0.0224, 0.0362, 0.0236, 0.0740, 0.0415],\n",
      "        [0.0378, 0.0310, 0.0542, 0.9821, 0.1138, 0.2330]], device='cuda:5')\n",
      "Train Epoch: 1\tscore:0.691189\t准确率：90.34%\n",
      "[1669/1670 (100%)]\t正确分类的样本数：35199，样本总数：40080，准确率：87.82%，score：0.7003803849220276\n",
      "epoch: 1_执行时间:  1181.8978979587555\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:1 end _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:2 start _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "labels: tensor([[0., 0., 0., 0., 0., 2.],.714977\t准确率：91.34%\n",
      "        [0., 0., 0., 0., 1., 0.]], device='cuda:5')\n",
      "pred: tensor([[0.1574, 0.1251, 0.0167, 0.0637, 0.0148, 0.1983],\n",
      "        [0.0138, 0.0131, 0.0156, 0.0305, 0.0517, 0.0520]], device='cuda:5')\n",
      "Train Epoch: 2\tscore:0.714973\t准确率：91.34%\n",
      "[1669/1670 (100%)]\t正确分类的样本数：35152，样本总数：40080，准确率：87.70%，score：0.6975222229957581\n",
      "epoch: 2_执行时间:  1135.1873180866241\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:2 end _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:3 start _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "labels: tensor([[0., 0., 0., 0., 0., 0.],.742551\t准确率：92.80%\n",
      "        [0., 0., 0., 0., 0., 0.]], device='cuda:5')\n",
      "pred: tensor([[0.0166, 0.0043, 0.0289, 0.0168, 0.1119, 0.1550],\n",
      "        [0.0354, 0.0659, 0.0482, 0.0167, 0.0102, 0.0079]], device='cuda:5')\n",
      "Train Epoch: 3\tscore:0.742594\t准确率：92.80%\n",
      "[1669/1670 (100%)]\t正确分类的样本数：34774，样本总数：40080，准确率：86.76%，score：0.6895523667335515\n",
      "epoch: 3_执行时间:  1117.0007195472717\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "_________________ epoch:3 end _________________\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import time  # 引入time模块\n",
    " \n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(device_train,device_test,epoch_be=0):\n",
    "    epoch_num=4\n",
    "    accumulation_steps=32\n",
    "    \n",
    "    train_score=[]\n",
    "    test_score=[]\n",
    "    test_acc=[]\n",
    "\n",
    "    for epoch_idx in range(epoch_be,epoch_be+epoch_num):\n",
    "        cls.to(device_train)\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________ epoch:\"+str(epoch_idx)+\" start _________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        score_list=[]\n",
    "        loss_a=0\n",
    "        total=0\n",
    "        correct=0\n",
    "        start = time.time()\n",
    "        for batch_idx,batch in enumerate(train_loader):\n",
    "            label6=torch.stack(batch['label']).t().to(device_test).float()\n",
    "            label18=torch.stack(batch['label18']).t().to(device_test).float()\n",
    "\n",
    "            #计算output\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device_train)\n",
    "            mask=torch.stack(batch['mask']).t().to(device_train)\n",
    "            output = cls(input_ids, attention_mask=mask)\n",
    "            output=sigmoid(output)\n",
    "            output6=torch.Tensor(_18to6_(output,round_=0)).to(device_train)\n",
    "\n",
    "\n",
    "            # 梯度积累\n",
    "            loss0 = criterion(output, label18)\n",
    "            lossh=criterion_hui(output6,label6)\n",
    "\n",
    "            loss=loss0+lossh\n",
    "            loss = loss/accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            #计算score\n",
    "            with torch.no_grad():\n",
    "                total += len(output)*6\n",
    "                loss_add,cor_add = get_loss_test(output, label6,round_=0)\n",
    "                loss_a+=loss_add\n",
    "                correct+=cor_add\n",
    "\n",
    "                tr_score=1/(1+(loss_a/total) ** 0.5)\n",
    "                acc_score=100.*correct/total\n",
    "\n",
    "\n",
    "            if((batch_idx+1) % accumulation_steps) == 0:\n",
    "                # 每 accumulation_steps 次更新一下网络中的参数\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if ((batch_idx+1) % accumulation_steps) == 1:\n",
    "                score_list.append(tr_score)\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tscore:{:.6f}\\t准确率：{:.2f}%'.format(\n",
    "                    epoch_idx, batch_idx, len(train_loader), \n",
    "                    100.*batch_idx/len(train_loader), tr_score,acc_score\n",
    "                ),end='\\r')\n",
    "\n",
    "        #每个epoch结束：\n",
    "        print('labels:', label6)\n",
    "        print('pred:', output6)\n",
    "#         plt.plot([i for i in range(len(score_list))], score_list)\n",
    "#         plt.show()\n",
    "\n",
    "        print('Train Epoch: {}\\tscore:{:.6f}\\t准确率：{:.2f}%'.format(epoch_idx,tr_score,acc_score))\n",
    "        t_score,t_acc=test(device_test)\n",
    "\n",
    "        train_score.append(tr_score)\n",
    "        test_score.append(t_score)\n",
    "        test_acc.append(t_acc)\n",
    "\n",
    "        #保存模型\n",
    "        end = time.time()\n",
    "        print('epoch:',str(epoch_idx)+'_执行时间: ',end - start)\n",
    "        torch.save(cls,\"../data/cls_\"+str(end)+\"_\"+str(epoch_idx)+\"_\"+str(round(tr_score.tolist(),4))+\"_\"+str(round(t_score.tolist(),4))+\".model\")\n",
    "\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________ epoch:\"+str(epoch_idx)+\" end _________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print(\"_________________________________________________________________\")\n",
    "\n",
    "#     plt.plot([i for i in range(len(train_score))], train_score)\n",
    "#     plt.show()             \n",
    "\n",
    "#     plt.plot([i for i in range(len(test_score))], test_score)\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.plot([i for i in range(len(test_acc))], test_acc)\n",
    "#     plt.show()\n",
    "    \n",
    "train(device0,device1,epoch_be=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "torch.save(cls,\"../data/cls_\"+str(end)+\"_\"+str(2)+\"_\"+str(0.6886)+\"_\"+str(0.6942)+\".model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "def get_output(device_pre,model,text,f=0):\n",
    "    model.to(device_pre)\n",
    "    model.eval()\n",
    "    text2id = tokenizer(\n",
    "        text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "#     print(text,text2id)\n",
    "    input_ids=text2id[\"input_ids\"].to(device_pre)\n",
    "    mask=text2id[\"attention_mask\"].to(device_pre)\n",
    "#         print(text2id)\n",
    "    output = model(input_ids, attention_mask=mask)\n",
    "    print(output)\n",
    "    output=sigmoid(output)\n",
    "    print(output)\n",
    "    \n",
    "#     output=sigmoid(output)*4\n",
    "    if f==1:\n",
    "        return _18to6_(output,round_=1)\n",
    "    else:\n",
    "        return _18to6_(output,round_=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.9952, -6.0604, -5.4647, -7.4934, -6.1120, -5.5436, -2.6384, -0.2311,\n",
      "          1.5884, -6.6223, -5.7481, -4.5401, -5.4886, -4.1617, -2.5640, -6.0817,\n",
      "         -5.3633, -4.8412]], device='cuda:5', grad_fn=<AddmmBackward0>)\n",
      "tensor([[9.1540e-04, 2.3281e-03, 4.2157e-03, 5.5645e-04, 2.2112e-03, 3.8972e-03,\n",
      "         6.6708e-02, 4.4248e-01, 8.3039e-01, 1.3286e-03, 3.1787e-03, 1.0560e-02,\n",
      "         4.1168e-03, 1.5342e-02, 7.1493e-02, 2.2791e-03, 4.6637e-03, 7.8357e-03]],\n",
      "       device='cuda:5', grad_fn=<SigmoidBackward0>)\n",
      "[[0.007875097915530205, 0.006997216492891312, 1.3971621990203857, 0.01565060205757618, 0.09330970048904419, 0.015700681135058403]]\n"
     ]
    }
   ],
   "source": [
    "text = ['*与n3：啊？']\n",
    "# 爱、乐、惊、怒、恐、哀\n",
    "print(get_output(device1,cls,text,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs0",
   "language": "python",
   "name": "bs0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
