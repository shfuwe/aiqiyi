{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ban=0\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device0 = torch.device('cuda:5' if torch.cuda.is_available() else \"cpu\")\n",
    "device1 = torch.device('cuda:5' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读数据\n",
    "import pandas as pd\n",
    "df_train=pd.read_excel(\"../data/df_train.xlsx\",index_col=0)\n",
    "df_test=pd.read_excel(\"../data/df_test.xlsx\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     天空下着暴雨，*正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。\n",
      "label                          [0, 0, 0, 0, 0, 0]\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df_train)):\n",
    "    print(df_train.loc[i])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "df_train_text=[df_train['text'][i] for i in range(len(df_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2id_train = tokenizer(\n",
    "        df_train_text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "input_ids_train=text2id_train[\"input_ids\"]\n",
    "mask_train=text2id_train[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train=input_ids_train.to(device0)\n",
    "mask_train=input_ids_train.to(device0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o2一手拿着一个行李，一路小跑着把*带到了文工团门口。 tensor([ 101,  157, 8144,  671, 2797, 2897, 4708,  671,  702, 6121, 3330, 8024,\n",
      "         671, 6662, 2207, 6651, 4708, 2828,  115, 2372, 1168,  749, 3152, 2339,\n",
      "        1730, 7305, 1366,  511,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "i=3\n",
    "print(df_train_text[i],input_ids_train[i])#115 *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o2一手拿着一个行李，一路小跑着把*带到了文工团门口。 tensor([ 101,  157, 8144,  671, 2797, 2897, 4708,  671,  702, 6121, 3330, 8024,\n",
      "         671, 6662, 2207, 6651, 4708, 2828,  115, 2372, 1168,  749, 3152, 2339,\n",
      "        1730, 7305, 1366,  511,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print(df_train_text[i],input_ids_train[i])#115 *\n",
    "print(mask_train[i])#115 *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o2一手拿着一个行李，一路小跑着把*带到了文工团门口。\n",
      "[CLS] o ##2 一 手 拿 着 一 个 行 李 ， 一 路 小 跑 着 把 * 带 到 了 文 工 团 门 口 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] tensor([ 1.6176e-01, -5.7589e-01, -4.1629e-01,  1.8868e-01, -6.9119e-01,\n",
      "        -1.4571e-01,  8.1848e-01, -1.6463e-01, -9.6849e-01,  8.0588e-01,\n",
      "         2.1206e-01, -4.0109e-01, -1.7737e-01, -4.8739e-01,  8.7368e-01,\n",
      "         5.7747e-01,  3.1207e-01, -2.9949e-01,  7.5629e-01, -6.8908e-02,\n",
      "         9.7767e-01,  1.1904e+00,  1.8328e-01, -6.5416e-01,  3.4930e-01,\n",
      "        -2.3717e-01,  4.6956e-01, -2.5943e-01, -2.0320e-01,  3.4010e-02,\n",
      "        -2.0179e-01, -2.2876e-02,  6.9094e-01,  6.9283e-01,  4.6820e-01,\n",
      "         2.0362e-01, -1.4426e+00,  3.1749e-02, -1.0237e-01, -5.2343e-01,\n",
      "         1.0269e+00,  9.8108e-01,  8.5352e-02, -8.1371e-01,  2.4699e-01,\n",
      "         1.5718e+00,  2.5285e-01, -3.6386e-02,  1.1915e+00,  3.7167e-01,\n",
      "         1.4232e-01,  8.3593e+00, -2.1009e+00, -1.3240e+00, -5.8046e-01,\n",
      "         2.6506e-01,  2.5169e-01, -1.3593e-01, -1.0558e+00,  1.1320e+00,\n",
      "        -6.9887e-01, -1.9317e-01,  1.2631e-01,  1.1896e+00, -1.3114e+00,\n",
      "        -5.9945e-01,  3.0446e-01,  2.1877e+00,  6.4182e-01,  8.5398e-01,\n",
      "        -4.4250e-01,  1.3813e-01, -7.9174e-01,  6.3451e-02,  2.1203e-01,\n",
      "        -6.4736e-01, -1.7066e-01, -1.5206e+00, -1.1640e+00, -1.7146e-01,\n",
      "        -3.3787e-01, -5.0049e-02, -3.5609e-01, -9.6657e-01, -1.1958e+00,\n",
      "         6.7065e-02,  4.0816e-01, -2.3951e-01, -9.7253e-01, -1.4601e+00,\n",
      "        -1.3880e-01, -2.4653e+00,  2.1275e+00, -2.2373e-01,  4.7147e-03,\n",
      "        -9.8900e-01, -1.1160e+00,  7.5235e-01,  9.0311e-02,  7.7367e-01,\n",
      "         8.3324e-01, -1.2835e+00, -7.0832e-01, -2.8757e-01,  4.9953e-01,\n",
      "        -1.1148e+00, -8.5089e-01,  1.4109e+00, -8.3444e-03, -6.5114e-01,\n",
      "         4.2536e-01,  2.9763e-01,  4.5225e-01, -6.6670e-01,  5.9270e-01,\n",
      "         6.7494e-01, -1.4596e+00,  2.9615e-01, -7.0622e-01, -1.0792e-01,\n",
      "        -7.2703e-01,  6.4639e-01,  9.2063e-01, -7.5091e-01,  3.8080e-01,\n",
      "        -9.5034e-01,  6.6379e-01, -3.5701e-01, -5.5610e-02,  1.5226e-01,\n",
      "         3.9117e-01, -1.1870e+00,  3.3112e-01, -8.5287e-02,  7.4672e-01,\n",
      "        -6.6655e-01, -7.2605e-01, -6.9166e-01,  4.4077e-01,  9.1843e-02,\n",
      "         4.1553e-01,  3.3621e-01,  1.2595e+00, -4.0419e-01, -4.3608e-01,\n",
      "        -8.2923e-01, -8.4035e-02, -1.0666e+00,  1.1809e-01,  1.0272e+00,\n",
      "         1.7595e+00,  1.8878e-01, -1.1321e+00,  2.4090e-01, -3.7143e-01,\n",
      "         8.7426e-01,  9.3391e-02,  1.5414e-01, -3.1792e-01,  2.2288e-01,\n",
      "        -5.5317e-01, -3.4091e-01, -7.2574e-01,  1.1931e+00,  8.6364e-01,\n",
      "         3.6113e-01, -1.0152e-01, -1.1195e+00,  3.9068e-01,  4.3532e-01,\n",
      "         1.3372e+00,  7.3345e-02, -1.0131e+00,  4.8391e-01,  4.2942e-01,\n",
      "         9.6947e-01, -1.2006e-01, -1.2652e+00,  3.6251e-01,  5.6188e-01,\n",
      "        -1.2699e+00,  5.7228e-01, -5.0056e-01,  8.1904e-01,  4.0272e-01,\n",
      "         8.4440e-01,  1.0776e+00,  7.0170e-02, -1.4562e-01, -1.0265e+00,\n",
      "        -1.9383e-01,  6.7059e-01, -1.1780e+00,  2.1450e-01,  1.9076e-01,\n",
      "        -1.0724e+00, -2.9107e-02,  2.7320e-01,  7.3552e-02, -7.3037e-01,\n",
      "        -5.1914e-01, -8.5025e-01,  6.0140e-01, -4.4035e-01,  1.1647e+00,\n",
      "        -6.6304e-02, -2.7671e-01,  7.2720e-02, -1.0208e+00, -1.1857e-02,\n",
      "        -2.4001e+00, -1.2687e+00,  1.1254e-01, -6.3408e-01,  1.5007e-01,\n",
      "        -1.2724e+00, -8.2658e-01,  2.6018e-01, -2.2606e-01, -2.2403e-01,\n",
      "         4.7416e-01, -5.5050e-01, -6.1149e-01,  4.4176e-01, -1.1332e+00,\n",
      "        -6.5180e-01,  2.3935e-01,  1.2631e-01,  7.6349e-02,  4.3875e-01,\n",
      "        -9.8435e-02,  1.9663e-01, -1.1188e-01, -1.7616e-01, -1.2389e+00,\n",
      "         1.0560e-01,  6.5142e-02, -1.0183e+00,  1.5712e+00,  4.3737e-01,\n",
      "         2.6643e-01,  1.1721e-01, -4.1958e-01,  4.4847e-01,  3.6318e-01,\n",
      "        -3.7197e-01, -3.9696e-01,  3.3680e-01, -4.7127e-01,  5.6503e-01,\n",
      "         1.9254e-01,  4.8412e-01,  7.9452e-01,  6.8604e-01, -6.4555e-01,\n",
      "        -9.8310e-02,  2.3925e-01, -2.0724e-01, -5.1983e-01,  7.5412e-01,\n",
      "        -6.0092e-01,  3.3671e-01, -8.3465e-01,  1.1216e+00,  5.6101e-01,\n",
      "         2.9736e-01,  3.8242e-01,  2.6104e-01,  4.4594e-01,  1.8298e-01,\n",
      "         7.0420e-01, -4.0062e-01, -9.0733e-02, -1.2092e+00, -3.7593e-03,\n",
      "        -3.4780e-01,  7.0600e-01,  5.5696e-01, -3.2551e-01, -1.0378e+00,\n",
      "        -3.1292e-01, -3.0073e-01,  2.1323e-01, -2.2700e-01, -4.5981e-01,\n",
      "         9.4965e-01,  3.4883e-01, -1.2155e-01,  2.2583e-01,  7.1904e-01,\n",
      "        -7.7297e-01, -9.3818e-01, -1.4142e+00,  8.1172e-01, -1.9791e-01,\n",
      "         1.1706e+00,  1.8240e-01,  7.8698e-01,  4.9128e-01,  3.9643e-01,\n",
      "         8.2206e-01, -4.0737e-01, -4.5805e-01,  6.4651e-02, -1.4237e-01,\n",
      "        -3.4703e-03,  6.5255e-01,  5.1490e-01,  1.0047e+00,  2.8855e-02,\n",
      "        -1.5329e+00, -4.6073e-01, -1.5268e+00, -8.1687e-01, -3.3004e+00,\n",
      "        -3.5689e-01,  3.5739e-01,  3.9573e-01,  1.0981e-01,  2.2216e-01,\n",
      "        -2.4254e-01,  7.5284e-01,  3.5233e-01,  1.1623e+00,  5.2798e-01,\n",
      "        -3.9553e-01,  2.1968e-01,  2.4639e-01,  5.0991e-01,  1.9532e-01,\n",
      "         6.4561e-02, -1.2309e+00,  4.3566e-01, -1.9395e-01, -7.5197e-01,\n",
      "        -8.6154e-01, -8.8246e-01, -7.4280e-01,  2.1768e-01, -1.0262e-01,\n",
      "         2.2578e-01, -5.6940e-02,  1.1916e+00, -1.2468e-01, -1.8788e-01,\n",
      "        -3.9693e-01, -4.1848e-01, -7.7766e-01,  6.2891e-02,  1.9057e+00,\n",
      "         9.3579e-02, -7.1248e-01, -7.0234e-01,  7.7403e-01,  8.4089e-01,\n",
      "         3.0623e-01,  2.6739e-01, -5.5423e-01, -1.1898e+00,  3.0211e-01,\n",
      "        -1.6515e-01, -2.8662e-01,  6.9744e-01, -4.7064e-01, -6.5214e-01,\n",
      "         3.2277e-01,  2.1516e-01, -7.7352e-01, -1.8409e+00,  4.1326e-01,\n",
      "        -2.8398e-02, -1.5361e-01, -1.5716e-01,  2.2125e-01,  7.0523e-01,\n",
      "        -6.4370e-02, -9.4079e-04, -2.0421e-01, -1.4453e+00,  1.6881e-01,\n",
      "         2.6885e-01, -9.5747e-01,  2.5510e-01, -8.3548e-01, -3.5896e-01,\n",
      "        -1.2070e-01, -3.6020e-01, -7.2077e-02,  6.4486e-01,  7.8635e-02,\n",
      "         3.6304e-01, -6.1573e-01, -3.1680e-01,  1.0234e-01,  3.3194e-02,\n",
      "        -1.1136e-01, -4.4112e-01, -4.5249e-01, -5.4619e-01,  2.3789e-01,\n",
      "         7.2035e-01,  6.8065e-01, -4.8960e-02,  7.4607e-01,  1.3847e-03,\n",
      "         4.8713e-02, -5.0337e-02, -6.1135e-01,  1.4169e+00,  6.8431e-01,\n",
      "         2.2761e-01, -1.5972e-02,  3.6776e-01, -2.2012e-01, -1.0582e+00,\n",
      "         1.5943e+00,  1.1759e+00,  3.7349e-01,  4.0846e-01, -1.1162e+00,\n",
      "        -3.6245e-01, -9.9335e-02, -1.3826e-01, -2.9838e-02, -7.6581e-01,\n",
      "        -9.0619e-02,  4.7717e-01,  1.3597e+00, -1.1730e-01, -1.2202e+00,\n",
      "         4.8466e-01,  3.6450e-01, -1.1306e-01, -3.4748e-01, -7.2987e-01,\n",
      "        -1.0308e+00, -1.5030e+00,  4.9597e-01,  3.8907e-02,  2.8762e-01,\n",
      "         4.5095e-01, -1.1782e+00, -7.2076e-01,  5.8928e-01,  7.4882e-01,\n",
      "         1.4759e+00,  1.9648e-01,  2.6884e-01,  4.4909e-01,  8.8202e-01,\n",
      "        -8.0667e-01,  1.0819e-01,  1.4304e+00, -8.3910e-03, -2.0059e-01,\n",
      "        -5.8305e-01,  4.6752e-01,  1.4003e+00,  1.1709e+00, -7.6107e-01,\n",
      "         2.8325e-01, -7.6886e-02, -1.1205e+00, -4.4135e-02,  1.5279e+00,\n",
      "         2.6441e-01, -9.3746e-01,  6.6643e-01,  4.1227e-01, -7.1822e-01,\n",
      "         5.3220e-01,  1.3383e-01, -1.2124e+00,  1.9654e-01,  5.6326e-01,\n",
      "        -6.9276e-01, -9.7224e-01, -6.0493e-04, -1.9018e+00, -1.6372e+00,\n",
      "         2.9849e-01,  1.0106e+00,  3.0670e-01, -3.5379e-01,  3.5911e-02,\n",
      "         1.1646e-01,  7.5535e-01, -1.6605e+00, -1.3712e-01,  3.3719e-01,\n",
      "        -5.4670e-01,  5.8281e-01,  5.9539e-01, -6.3140e-01, -4.9435e-01,\n",
      "        -7.9258e-01,  6.0282e-01, -3.3494e-01, -1.4772e+00,  1.1389e+00,\n",
      "        -3.7873e-03,  2.5840e-01,  6.4165e-02,  1.0487e+00, -7.0212e-01,\n",
      "        -3.5654e-01,  5.9447e-01,  8.1109e-01,  2.2652e-01,  2.0248e-01,\n",
      "         8.4771e-01,  4.0782e-01,  1.5643e+00, -1.0511e+00,  9.7035e-01,\n",
      "         1.6210e-01, -8.4535e-01, -4.8880e-01, -2.2768e-01,  4.1345e-01,\n",
      "        -9.4578e-01,  5.5818e-01, -1.6471e-01, -4.9136e-01, -4.5825e-01,\n",
      "         1.6451e-01, -4.1478e-01, -1.5459e+00, -3.5874e-01,  1.6244e-01,\n",
      "         6.8155e-01,  7.7923e-01, -4.6504e-01,  4.5952e-01,  3.9222e-01,\n",
      "        -1.1950e+00, -6.0323e-01, -7.8188e-01, -1.1189e+00, -3.5841e-01,\n",
      "        -3.5188e-01,  6.2678e-02,  2.5725e-01, -1.0256e-01, -7.3486e-01,\n",
      "         4.4724e-01, -1.3017e+00, -8.5844e-01,  5.2104e-01, -8.2633e-01,\n",
      "         1.1239e+00,  5.9307e-01,  4.7307e-01, -1.0731e+00, -7.8049e-01,\n",
      "         2.6824e-01,  4.5841e-01, -1.1689e+00,  1.3376e+00, -1.1582e-01,\n",
      "        -2.4219e-02, -2.5977e-01, -1.5550e+00,  6.9069e-02,  2.1646e-01,\n",
      "        -8.7768e-01, -6.3054e-01,  3.1104e-01, -3.2095e-01, -5.8987e-01,\n",
      "        -1.3810e+00,  1.2284e-01,  6.3586e-01,  1.3266e-01,  3.8667e-02,\n",
      "         2.0594e-01, -8.9858e-02, -4.4476e-01,  1.2033e+00,  1.1923e-01,\n",
      "         6.0831e-01,  1.1152e+00, -3.0144e-01, -4.2418e-01, -4.5244e-01,\n",
      "        -7.9450e-01, -4.2359e-01, -9.6238e-01,  9.8221e-01,  3.1809e-01,\n",
      "        -4.3561e-01, -5.1216e-01,  2.2463e-01,  3.5186e-01, -1.6248e-01,\n",
      "         5.6097e-01, -1.7525e+00, -6.3084e-02,  1.2836e-01,  7.8813e-01,\n",
      "         3.5464e-01, -8.4547e-02,  2.0568e-01,  1.2308e+00, -1.2107e+00,\n",
      "        -2.6978e-01,  6.5715e-01, -1.1423e+00,  8.9722e-01, -3.7381e-01,\n",
      "        -1.0832e+00, -2.7154e-01, -1.8470e-01,  1.5701e+00, -5.8102e-01,\n",
      "         8.9611e-01,  2.8922e-01,  1.9456e+00,  3.7437e-01, -1.1683e+00,\n",
      "        -4.2636e-01, -1.0644e-01, -6.3119e-01, -4.5397e-01, -2.7493e-01,\n",
      "         1.9660e-01,  1.0377e+00,  5.9151e-01,  7.8262e-01,  5.8747e-01,\n",
      "         3.9946e-01,  1.0741e-01,  9.7567e-01, -5.5619e-01, -4.4120e-02,\n",
      "        -1.1224e+00, -6.7113e-01,  1.0719e+00, -2.4821e-02,  8.6128e-01,\n",
      "         5.0135e-01, -1.2018e+00,  4.3818e-01, -3.1683e-01, -8.9081e-01,\n",
      "         3.5128e-01, -7.9425e-01, -7.7370e-02, -1.9862e-02,  8.5371e-02,\n",
      "         7.2096e-02,  2.0151e-01, -8.3369e-02, -3.4115e-01,  7.5279e-01,\n",
      "        -5.7846e-01, -3.7288e-01, -3.1968e-01,  3.5927e-01, -1.9943e+00,\n",
      "         5.2767e-01, -1.7362e-01, -4.4047e-01,  8.6707e-01,  6.7895e-01,\n",
      "         4.8620e-01, -2.3359e-01,  4.1290e-01, -8.1674e-01,  4.4912e-01,\n",
      "         1.8046e+00, -6.9420e-01,  5.8986e-01, -8.6166e-01, -8.2094e-01,\n",
      "        -6.5534e-01,  1.1578e+00, -3.7500e-01,  1.1880e+00,  3.0197e-01,\n",
      "         1.3016e+00,  2.3876e-01,  2.8263e-01,  1.2574e+00,  4.6497e-01,\n",
      "        -6.6123e-01,  1.3013e-01,  8.1743e-01,  2.3034e-01,  1.5390e-01,\n",
      "         8.1371e-01, -1.1766e+00, -2.8813e-01, -9.8074e-01,  6.4817e-01,\n",
      "         5.7197e+00, -9.0482e-02, -3.7966e-01,  3.8715e-01,  3.2913e-01,\n",
      "         1.4310e-01, -3.3238e-01, -7.3150e-01, -4.5601e-01,  3.2481e-02,\n",
      "         8.0511e-02, -3.2950e-01,  7.1272e-01, -6.4281e-01,  3.1553e-01,\n",
      "         4.8106e-01,  1.8828e-01,  1.7355e-01, -5.2107e-01,  8.1220e-01,\n",
      "         3.3029e-01,  1.0734e-01,  1.9341e-01,  4.6669e-01, -3.3357e-01,\n",
      "         5.0593e-01,  4.8084e-04, -3.2997e-02, -3.4036e-01,  4.3488e-01,\n",
      "         1.3181e-01, -8.0462e-01,  2.0700e-01, -2.6777e-01,  1.5845e-01,\n",
      "        -1.0994e+00, -1.1473e+00,  1.4189e+00, -8.3195e-01, -1.3691e+00,\n",
      "         2.3339e+00,  2.5185e-02,  8.1109e-01, -1.3400e-02, -3.8539e-01,\n",
      "        -8.3495e-01, -7.9320e-01,  6.4965e-01,  4.2708e-01,  3.7094e-01,\n",
      "        -6.6133e-01,  7.7881e-01,  2.8027e-01, -6.2630e-01,  5.7149e-01,\n",
      "        -2.7355e-01,  1.9235e-01, -9.9497e-01, -2.0594e-01, -2.8276e-01,\n",
      "         8.7211e-01, -1.6243e-01, -3.9052e-01,  9.4653e-01,  1.7196e+00,\n",
      "         1.5064e+00, -7.8387e-01,  1.5023e+00,  7.0176e-01,  4.6825e-01,\n",
      "         5.8795e-01, -5.6552e-01, -3.1841e-01], device='cuda:5',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "i=3\n",
    "print(df_train_text[i])\n",
    "for ids in input_ids_train[i]:\n",
    "    print(tokenizer.decode(ids),end=' ')\n",
    "ee=model(input_ids_train[i].view(1,-1), attention_mask=mask_train[i].view(1,-1))\n",
    "# print(ee)\n",
    "print(ee[0][0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*一手拿着一个行李，一路小跑着把c1带到了文工团门口。\n",
      "[CLS] * 一 手 拿 着 一 个 行 李 ， 一 路 小 跑 着 把 c1 带 到 了 文 工 团 门 口 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] tensor([-9.4848e-01,  1.1805e-01, -4.0959e-01,  5.3761e-01,  3.8722e-01,\n",
      "        -6.9740e-01, -8.5211e-01,  1.2992e+00,  2.1928e-01,  5.3818e-01,\n",
      "         4.6333e-01, -6.6036e-01,  1.3441e+00, -1.0664e+00, -4.1901e-01,\n",
      "         2.0003e-01,  3.9185e-01,  5.7158e-01, -2.5513e-01,  4.3548e-01,\n",
      "         8.4873e-01,  1.4879e+00,  8.4059e-01, -4.0989e-01, -1.4731e-01,\n",
      "         7.5528e-01, -5.5838e-02, -2.1050e+00, -8.4114e-03,  6.4950e-01,\n",
      "        -2.5560e-01,  5.3908e-01,  2.4499e-01,  1.5611e+00,  1.0899e+00,\n",
      "         2.4846e-01, -5.1498e-01, -2.7679e-01,  1.1131e+00, -4.1475e-01,\n",
      "         1.9348e-01,  4.0871e-02, -6.2132e-01,  3.5202e-02,  3.8474e-02,\n",
      "         1.0396e+00, -2.8810e-01, -9.3758e-02,  2.5193e-01, -2.6741e-01,\n",
      "        -4.9560e-01,  9.8093e+00, -2.9075e-01, -1.0283e+00, -4.2650e-01,\n",
      "         8.3085e-01, -4.3426e-01, -2.6406e-01, -1.1824e+00, -2.6143e-03,\n",
      "         1.0137e+00, -3.1587e-01,  1.0630e-01, -3.7657e-01, -3.7425e-01,\n",
      "         6.6858e-01,  7.2932e-01,  3.3780e-01,  9.7348e-01, -6.0148e-01,\n",
      "         3.9097e-01, -9.8367e-01,  5.0515e-01,  3.2606e-02,  9.2340e-02,\n",
      "         8.2775e-01,  2.6183e-01, -1.5415e+00, -4.7563e-01,  3.9451e-01,\n",
      "        -6.6379e-02, -1.3050e-01, -9.5308e-01, -7.8605e-02,  8.5426e-01,\n",
      "         3.0831e-02, -2.4896e-01,  9.1292e-02, -7.8790e-01, -4.5252e-01,\n",
      "        -4.1219e-01, -9.9290e-01,  4.4478e-02,  3.7361e-01, -9.8091e-02,\n",
      "         5.2552e-01,  7.9537e-02, -9.0761e-01,  1.0773e+00, -7.3017e-01,\n",
      "         6.6977e-01, -1.5681e+00, -2.4417e-01, -5.9227e-01,  2.7058e-01,\n",
      "         3.5721e-01, -4.2418e-02, -1.4516e-01, -5.0558e-01, -6.1232e-01,\n",
      "        -9.2872e-01,  6.7949e-02,  8.4455e-02,  9.7712e-01, -9.9564e-01,\n",
      "        -1.9168e-01, -5.7155e-01, -1.3573e-01, -4.8160e-01,  8.9250e-02,\n",
      "         1.4837e-01, -5.7703e-01,  1.1397e+00, -7.7519e-01, -1.3749e-01,\n",
      "        -1.7484e-01,  9.9061e-01,  1.5420e-01,  4.5978e-01, -3.8150e-01,\n",
      "        -7.5626e-01, -6.8937e-01, -3.6530e-01, -1.1677e-01, -2.0349e+00,\n",
      "        -4.3761e-01,  2.8988e-02, -1.5462e+00, -4.3350e-01, -5.9330e-01,\n",
      "        -1.0176e+00, -4.1274e-01, -1.0557e+00, -7.0118e-01,  5.7722e-01,\n",
      "        -1.3882e+00, -6.1019e-01, -3.3431e-01, -9.5165e-01,  6.5415e-01,\n",
      "         1.6134e-01,  1.1640e+00,  2.9439e-01, -5.4529e-01, -3.4290e-01,\n",
      "         8.1182e-01,  1.2345e+00,  5.2507e-01,  3.4256e-01,  4.6281e-01,\n",
      "        -7.7689e-01, -1.9951e-01,  1.9151e-02, -3.3347e-01,  3.3551e-01,\n",
      "         6.4407e-01,  3.1059e-01, -1.2127e-01,  4.2862e-01,  9.1676e-01,\n",
      "         3.1950e-01, -6.9179e-01, -1.3887e-01,  1.4221e+00, -1.5114e-01,\n",
      "         1.6109e+00,  5.7199e-01,  2.4963e-01,  1.2658e+00, -4.4377e-01,\n",
      "        -1.9290e-02, -2.6964e-01, -2.4525e-01, -6.1987e-01,  1.0764e+00,\n",
      "        -7.4460e-01,  4.9299e-01, -4.3526e-01,  3.4235e-01,  6.9943e-01,\n",
      "        -8.9877e-01,  2.8608e-01,  2.2612e-01, -6.5647e-01, -1.1091e+00,\n",
      "        -4.4793e-02,  5.2202e-01,  9.3581e-01,  6.0731e-01, -2.7910e-01,\n",
      "         3.8248e-01,  2.2856e-01, -7.7816e-01,  5.0222e-01, -3.1130e-01,\n",
      "         5.8957e-01, -2.2887e-01, -1.4760e-02, -1.0663e+00, -4.6748e-01,\n",
      "        -8.8150e-01,  6.1758e-01, -1.3566e-01,  7.0701e-02,  3.8581e-01,\n",
      "        -7.6317e-01, -1.8772e-01,  6.4022e-01, -8.4633e-01, -1.6545e-01,\n",
      "         4.4342e-01, -9.4762e-02,  5.7128e-01, -7.8812e-01, -3.3404e-01,\n",
      "        -6.2535e-01, -1.5048e-01,  2.6923e-01, -3.5781e-01, -1.0549e-01,\n",
      "         7.6980e-01,  6.0007e-01,  1.0396e+00,  3.7178e-01, -8.0821e-01,\n",
      "        -3.0266e-01, -2.6620e-01, -1.6837e+00, -5.3156e-01, -1.1785e+00,\n",
      "        -2.0642e-01, -6.5603e-01, -6.0946e-01,  3.4510e-01, -1.4361e+00,\n",
      "        -6.2090e-02, -1.5456e-01, -8.1960e-01, -1.1356e+00,  1.5911e+00,\n",
      "        -1.3908e+00, -8.7311e-03, -6.9262e-01, -1.0043e+00, -4.1516e-01,\n",
      "        -1.2716e+00, -9.7329e-01, -2.4534e-01,  2.0504e-01, -1.1391e+00,\n",
      "         3.9922e-01, -3.6911e-01,  3.4242e-01, -1.5601e-01,  1.2689e+00,\n",
      "         2.7069e-01,  1.3305e+00, -1.7760e-01,  5.7220e-01,  5.7024e-01,\n",
      "        -7.3851e-01, -2.5752e-01, -1.1780e-01, -2.0387e+00, -5.0008e-01,\n",
      "        -1.7072e-01,  6.0986e-01,  6.7255e-01, -4.2230e-01, -7.2853e-01,\n",
      "         9.0132e-01, -1.0229e+00, -5.9175e-01,  7.9144e-01, -2.4335e-01,\n",
      "        -7.0242e-01,  3.0050e-01, -9.8630e-01,  4.0858e-01,  2.3233e-03,\n",
      "         4.1579e-02, -5.4959e-01, -2.7887e-01,  8.5242e-02,  1.9482e-01,\n",
      "         5.5282e-01, -9.8338e-02, -7.6783e-01, -2.9859e-01,  3.8621e-01,\n",
      "         4.8046e-01, -8.9763e-02, -1.0955e+00,  1.4339e+00, -7.3427e-02,\n",
      "        -7.6463e-01, -3.6960e-01,  4.9907e-01,  4.5641e-01,  5.6722e-01,\n",
      "        -7.7770e-02, -6.5037e-01,  2.6848e-01,  3.2335e-01, -2.4834e+00,\n",
      "         1.3555e-02,  1.6879e+00, -6.7483e-01,  2.0804e-01,  1.0275e+00,\n",
      "         3.2028e-01,  4.7472e-02,  6.4087e-01,  2.8130e-01,  6.0547e-01,\n",
      "        -2.1153e-01, -7.1739e-01,  3.9897e-01,  5.7859e-01,  6.4838e-01,\n",
      "         4.1995e-01,  1.9366e-01, -4.5552e-02, -1.1105e+00,  6.8002e-01,\n",
      "         5.1608e-01,  4.1045e-01, -1.8472e-01, -1.1830e-01, -6.8740e-01,\n",
      "         1.8355e-01, -5.5952e-01,  1.3050e+00, -4.7234e-01, -1.5139e+00,\n",
      "        -1.4114e+00, -2.4961e+00, -1.7046e+00, -5.6463e-02, -6.5587e-02,\n",
      "        -4.1154e-01, -1.7204e+00, -5.3338e-01,  9.4756e-01, -5.2160e-01,\n",
      "         6.1115e-01,  1.6273e+00,  3.2470e-01,  1.7181e-01,  4.6847e-03,\n",
      "         7.5111e-01, -5.7240e-01,  5.0796e-01,  8.8351e-01, -3.4452e-03,\n",
      "        -1.7259e-01,  7.9496e-01, -1.9712e-01, -4.4815e-01,  4.7400e-01,\n",
      "        -7.0085e-02, -1.1552e+00, -2.0983e-01, -2.1636e-02,  5.4253e-01,\n",
      "        -6.9618e-01, -8.3534e-01,  3.1370e-01, -1.0724e-01, -4.8749e-01,\n",
      "         1.2006e+00,  6.7377e-01, -5.9664e-01, -1.1261e+00, -6.6925e-01,\n",
      "         7.8071e-02,  1.6726e-01,  9.8134e-02,  3.2790e-01, -6.4072e-01,\n",
      "        -1.3232e-01, -3.0699e-01,  5.7422e-01, -2.1709e-01,  5.0677e-01,\n",
      "         3.4931e-01, -6.3136e-01,  7.3316e-01, -1.3026e+00, -3.5646e-01,\n",
      "        -1.2528e-01, -1.2053e-01, -1.5502e-01,  1.2135e+00,  7.2294e-01,\n",
      "         1.1531e-01, -1.0415e-01,  5.8385e-02,  6.9633e-01,  8.9162e-01,\n",
      "         1.1546e-01, -1.9810e+00, -9.8102e-01,  1.7440e-01,  6.2088e-01,\n",
      "         2.4508e-01,  4.1404e-01, -3.7725e-01,  7.2821e-01,  1.9117e-02,\n",
      "        -5.6681e-01,  2.1104e-01,  6.1400e-01,  9.0301e-01, -5.4460e-01,\n",
      "         1.4557e+00, -6.7303e-01, -7.6939e-02, -8.8843e-01, -8.5299e-02,\n",
      "        -3.5209e-01,  4.9125e-02, -2.5348e-01, -9.1896e-02, -1.0686e-03,\n",
      "         3.6969e-01,  3.4116e-01, -4.2804e-01,  3.4596e-02,  5.8172e-01,\n",
      "        -5.1410e-01, -7.7338e-01, -3.4937e-01, -3.3852e-01,  3.9676e-01,\n",
      "         3.1913e-01,  5.2588e-01,  5.7614e-01,  1.3961e-01, -3.4560e-01,\n",
      "        -1.5720e+00,  2.1184e-02,  7.7551e-01, -1.1680e+00, -4.6629e-01,\n",
      "        -5.8905e-01, -1.4442e+00, -6.1245e-01,  1.1326e+00, -6.9434e-01,\n",
      "         4.1804e-01,  6.3498e-01,  2.3998e-01, -1.6054e-01, -5.0558e-01,\n",
      "        -7.2101e-01, -1.5152e+00, -2.8515e-01,  2.0274e-01, -1.0044e-01,\n",
      "        -1.4091e-01,  2.7070e-01, -1.9394e+00,  5.9800e-01, -6.0716e-01,\n",
      "         7.8958e-01,  6.1568e-01, -5.3587e-01, -3.9771e-01, -4.0660e-02,\n",
      "        -1.6924e-01,  1.4069e-01,  1.0302e+00,  2.5866e-01, -3.0632e-01,\n",
      "        -2.8321e-01,  1.6943e-01, -2.3273e-01,  1.6996e-01,  2.1025e-01,\n",
      "         2.8458e-01,  8.2143e-01,  1.0534e+00, -8.5029e-01, -6.2745e-01,\n",
      "        -2.6265e-01, -2.5887e-01,  1.0512e-01, -9.2185e-01,  9.3234e-01,\n",
      "        -7.1114e-02, -4.5866e-01, -2.1992e-02,  8.3636e-01,  9.4646e-01,\n",
      "         8.0793e-01,  1.4865e+00, -7.5417e-01, -8.2591e-01, -7.1404e-01,\n",
      "         1.1214e+00, -4.4219e-01, -4.3267e-01,  1.3831e+00,  1.2201e+00,\n",
      "         2.2475e-01,  3.9784e-02, -3.3258e-01, -2.4476e-01,  9.6565e-01,\n",
      "        -3.2135e-01, -1.9512e-01,  4.5973e-01, -1.4440e-01, -6.0116e-01,\n",
      "         5.5731e-01, -3.0457e-01, -1.3727e-01,  1.3289e+00,  5.2136e-01,\n",
      "        -3.3895e-01, -1.4012e-01,  6.9342e-01,  1.7528e+00,  1.9720e-01,\n",
      "         4.9918e-01,  1.0952e+00, -5.0491e-01, -1.5083e+00, -2.9910e-01,\n",
      "        -2.0524e-01, -7.0561e-01,  2.7327e-01, -3.9314e-01, -6.4580e-01,\n",
      "        -1.2115e+00, -1.0584e+00,  3.7757e-01,  2.8101e-01, -6.4859e-01,\n",
      "        -1.5570e+00,  3.4760e-01,  9.1435e-01, -1.5601e-01, -2.1431e-01,\n",
      "        -1.2324e+00,  6.9668e-01, -2.4647e-01,  2.4785e-01,  9.8184e-01,\n",
      "         1.1735e+00, -3.7069e-01, -3.2338e-01, -1.0497e+00, -2.3118e-01,\n",
      "        -6.0617e-01, -5.3394e-01, -4.6111e-03, -5.4774e-01, -1.3291e+00,\n",
      "        -9.0000e-01, -5.2899e-01,  1.2738e-01,  8.8924e-01, -5.1011e-01,\n",
      "         1.3856e+00,  1.3946e-01,  6.5111e-01,  6.7023e-01, -3.8986e-02,\n",
      "         1.0685e+00,  1.3364e-01,  8.6147e-02, -2.1184e-01, -3.0383e-01,\n",
      "        -1.0352e+00, -2.3373e-01,  6.8804e-01, -1.0090e-01, -2.2517e-01,\n",
      "        -3.4223e-01,  2.7053e-01,  3.9458e-02, -5.8357e-01,  5.5623e-01,\n",
      "        -5.0254e-01, -5.6017e-01,  3.5619e-01, -1.0326e+00,  8.7186e-01,\n",
      "        -2.0656e-01, -3.9711e-01,  1.1170e+00, -5.9038e-02,  8.4262e-02,\n",
      "        -9.7664e-02,  1.1282e-01, -3.5880e-01, -7.0403e-01, -7.7341e-01,\n",
      "        -7.3132e-01, -1.4409e+00, -4.4160e-02,  2.9135e-01, -4.9928e-02,\n",
      "         3.1886e-01, -1.1736e-01,  2.1660e-01,  1.0688e-01, -3.8217e-02,\n",
      "        -1.7009e-01,  3.5373e-01, -1.2935e+00,  2.6018e-01,  1.8256e-01,\n",
      "         1.1076e+00, -3.1758e-01, -4.5743e-01,  2.5947e-02, -4.8306e-03,\n",
      "        -2.5653e-01,  4.8523e-01, -3.8799e-01, -2.6576e-01, -6.3426e-01,\n",
      "        -3.1812e-01, -8.7694e-02,  3.1174e-01, -1.6678e-01, -4.5707e-01,\n",
      "        -6.7037e-02, -1.2962e+00, -1.8357e-01, -4.4939e-01, -1.4800e-01,\n",
      "         1.1773e+00,  2.7917e-01,  3.4394e-02,  1.6887e+00,  1.1494e-01,\n",
      "         3.6983e-01,  1.1070e+00, -2.4224e-01, -9.9188e-01,  6.0897e-01,\n",
      "         3.5429e-01,  9.1321e-01, -5.1210e-01,  6.1823e-01,  8.2662e-01,\n",
      "         3.6861e-01, -6.7964e-01, -9.2029e-01, -3.8421e-01,  1.2307e+00,\n",
      "         9.1846e-01, -9.3844e-01,  1.0743e-02,  1.2257e-01,  5.2912e-01,\n",
      "         3.2813e-01, -2.1338e-01,  1.4808e+00, -1.7969e+00,  9.8132e-02,\n",
      "         3.2220e-01,  1.8785e-01,  4.5031e-01,  1.6639e+00,  1.0474e+00,\n",
      "         3.6574e-04,  1.1228e+00, -5.5494e-01,  9.4112e-01, -5.1430e-01,\n",
      "         1.1251e-01, -8.1615e-01,  3.2492e-01,  1.8985e+00, -6.7685e-01,\n",
      "         1.0101e+00, -9.9694e-01,  3.7472e-01, -4.1493e-01,  4.3528e-01,\n",
      "         4.2521e+00,  5.4352e-01, -5.5352e-01, -4.5248e-01,  1.3980e+00,\n",
      "        -2.5202e-01, -2.8052e-01, -2.4297e-01, -2.1594e-01, -6.8361e-01,\n",
      "         1.0069e+00, -2.0941e-01,  3.8557e-01,  2.5153e-01,  2.7230e-01,\n",
      "        -8.7243e-01,  9.3450e-01,  1.2684e+00,  1.0474e+00,  2.9771e-01,\n",
      "         3.3702e-01, -6.9298e-02, -5.2612e-01, -4.2210e-01,  2.7165e-01,\n",
      "         4.3822e-01,  1.2834e-01,  6.5549e-01,  3.5263e-01,  2.6205e-01,\n",
      "         5.5785e-02, -1.0788e+00,  8.0312e-01,  3.7754e-01, -1.1565e-01,\n",
      "        -3.6325e-01,  6.5267e-01,  1.2775e+00,  5.3910e-02, -5.9193e-01,\n",
      "         1.1875e-01, -1.1372e+00,  7.1145e-02,  3.1222e-01,  7.5724e-03,\n",
      "         6.6211e-01, -6.0377e-01,  6.1119e-01,  5.4507e-02,  1.1672e+00,\n",
      "        -1.6461e-03, -1.1731e+00, -1.7425e-01,  6.9175e-01,  4.9590e-01,\n",
      "        -7.4627e-01, -1.4880e+00,  3.5197e-01, -5.5117e-02,  3.5217e-01,\n",
      "        -2.5394e-01,  1.2323e+00,  1.7227e-01,  9.3402e-03,  5.8355e-01,\n",
      "        -4.2935e-01,  3.2166e-01, -9.9281e-02,  1.1352e+00, -5.6240e-01,\n",
      "         8.5729e-01, -9.8280e-01,  2.3584e-01], device='cuda:5',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "i=2\n",
    "print(df_train_text[i])\n",
    "for ids in input_ids_train[i]:\n",
    "    print(tokenizer.decode(ids),end=' ')\n",
    "ee=model(input_ids_train[i].view(1,-1), attention_mask=mask_train[i].view(1,-1))\n",
    "print(ee[0][0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "        self.model.to(device)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, 18)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "        x = outputs[0]#batch size* 100 * 768\n",
    "        x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.7741935483870968, 0.8064516129032258, 0.8387096774193549, 0.8709677419354839, 0.9032258064516129, 0.9354838709677419, 0.967741935483871, 1.0, 0.967741935483871, 0.9354838709677419, 0.9032258064516129, 0.8709677419354839, 0.8387096774193549, 0.8064516129032258, 0.7741935483870968, 0.7419354838709677, 0.7096774193548387, 0.6774193548387096, 0.6451612903225806, 0.6129032258064516, 0.5806451612903226, 0.5483870967741935, 0.5161290322580645, 0.4838709677419355, 0.45161290322580644, 0.41935483870967744, 0.3870967741935484, 0.3548387096774194, 0.3225806451612903, 0.2903225806451613, 0.25806451612903225, 0.22580645161290322, 0.1935483870967742, 0.16129032258064516, 0.12903225806451613, 0.0967741935483871, 0.06451612903225806, 0.03225806451612903, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 100\n"
     ]
    }
   ],
   "source": [
    "def get_att(input_ids_train_):\n",
    "    index=0\n",
    "    _115=[]\n",
    "    for i in input_ids_train_:\n",
    "        if i==115:\n",
    "#             print(i)\n",
    "            _115.append(index)\n",
    "        index+=1\n",
    "    ###########################################################\n",
    "    att=[]\n",
    "\n",
    "    index=0\n",
    "    max_=0\n",
    "    for i in input_ids_train[i]:\n",
    "        if i in [101,102,0]:\n",
    "            att.append(-1)\n",
    "            index+=1\n",
    "            continue\n",
    "        att_t=[]\n",
    "        for j in _115:\n",
    "            att_t.append(abs(index-j))\n",
    "\n",
    "\n",
    "        att.append(min(att_t))\n",
    "        if min(att_t)>max_:\n",
    "            max_=min(att_t)\n",
    "\n",
    "        index+=1\n",
    "#     print(att)\n",
    "    ###########################################################\n",
    "    att0=[]\n",
    "    for j in att:\n",
    "        if j==-1:\n",
    "            att0.append(0)\n",
    "        else:\n",
    "            att0.append((max_-j)/max_)\n",
    "#     print(att0)\n",
    "    return att0\n",
    "\n",
    "print(get_att(input_ids_train[i]),len(get_att(input_ids_train[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2,4,6)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8240, 0.6485, 0.9352, 0.1248],\n",
       "        [0.6806, 0.9089, 0.7867, 0.5594]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=torch.rand(2,4)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8240],\n",
      "        [0.6485],\n",
      "        [0.9352],\n",
      "        [0.1248]])\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0.8240, 0.8240, 0.8240, 0.8240, 0.8240, 0.8240],\n",
      "        [0.6485, 0.6485, 0.6485, 0.6485, 0.6485, 0.6485],\n",
      "        [0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352],\n",
      "        [0.1248, 0.1248, 0.1248, 0.1248, 0.1248, 0.1248]])\n",
      "tensor([2.5325, 2.5325, 2.5325, 2.5325, 2.5325, 2.5325])\n",
      "tensor([[0.6806],\n",
      "        [0.9089],\n",
      "        [0.7867],\n",
      "        [0.5594]])\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0.6806, 0.6806, 0.6806, 0.6806, 0.6806, 0.6806],\n",
      "        [0.9089, 0.9089, 0.9089, 0.9089, 0.9089, 0.9089],\n",
      "        [0.7867, 0.7867, 0.7867, 0.7867, 0.7867, 0.7867],\n",
      "        [0.5594, 0.5594, 0.5594, 0.5594, 0.5594, 0.5594]])\n",
      "tensor([2.9355, 2.9355, 2.9355, 2.9355, 2.9355, 2.9355])\n",
      "tensor([[2.5325, 2.5325, 2.5325, 2.5325, 2.5325, 2.5325],\n",
      "        [2.9355, 2.9355, 2.9355, 2.9355, 2.9355, 2.9355]])\n"
     ]
    }
   ],
   "source": [
    "all_e=[]\n",
    "for i in range(len(b)):\n",
    "    bb=b[i]\n",
    "    aa=a[i]\n",
    "    print(bb.view(4,1))\n",
    "    print(aa)\n",
    "    cc=aa*bb.view(4,1)\n",
    "    print(cc)\n",
    "    print(torch.sum(cc,0))\n",
    "    all_e.append(torch.sum(cc,0))\n",
    "print(torch.stack(all_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(2,6)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8189, 0.7499])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "b = torch.rand(2)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30407/4270369334.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "c=a/b.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0555, 0.6183, 0.0394, 0.9393, 0.7659, 0.9817, 0.2645, 0.7399, 0.4441,\n",
      "        0.0349, 0.8468, 0.5814, 0.3220, 0.6193, 0.5564, 0.0424, 0.9073, 0.1680,\n",
      "        0.1592, 0.5378, 0.9052, 0.0583, 0.7725, 0.3413])\n",
      "(tensor([0.0555, 0.6183, 0.0394, 0.9393]), tensor([0.7659, 0.9817, 0.2645, 0.7399]), tensor([0.4441, 0.0349, 0.8468, 0.5814]), tensor([0.3220, 0.6193, 0.5564, 0.0424]), tensor([0.9073, 0.1680, 0.1592, 0.5378]), tensor([0.9052, 0.0583, 0.7725, 0.3413]))\n",
      "tensor([[0.0555, 0.6183, 0.0394, 0.9393],\n",
      "        [0.7659, 0.9817, 0.2645, 0.7399],\n",
      "        [0.4441, 0.0349, 0.8468, 0.5814],\n",
      "        [0.3220, 0.6193, 0.5564, 0.0424],\n",
      "        [0.9073, 0.1680, 0.1592, 0.5378],\n",
      "        [0.9052, 0.0583, 0.7725, 0.3413]])\n",
      "tensor([[0.1624, 0.2850, 0.1597, 0.3929],\n",
      "        [0.2617, 0.3248, 0.1585, 0.2550],\n",
      "        [0.2322, 0.1542, 0.3473, 0.2663],\n",
      "        [0.2290, 0.3083, 0.2895, 0.1732],\n",
      "        [0.3785, 0.1807, 0.1791, 0.2616],\n",
      "        [0.3480, 0.1492, 0.3048, 0.1980]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "b = torch.rand(24)\n",
    "print(b)\n",
    "b0=b.split(4, 0)\n",
    "print(b0)\n",
    "b0=torch.stack(b0)\n",
    "print(b0)\n",
    "f = nn.Softmax(dim = 1) \n",
    "b1 = f(b0)\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2711, 0.8774, 0.0171, 0.2485, 0.1644, 0.9675, 0.1289, 0.2879, 0.1195,\n",
      "         0.3633, 0.2045, 0.3651, 0.9948, 0.1026, 0.8682, 0.8488, 0.1908, 0.5127,\n",
      "         0.2769, 0.1463, 0.2324, 0.4461, 0.0231, 0.6756],\n",
      "        [0.0144, 0.5674, 0.1124, 0.1504, 0.7175, 0.6074, 0.3690, 0.9623, 0.4468,\n",
      "         0.8777, 0.1288, 0.8800, 0.9503, 0.4536, 0.5544, 0.0623, 0.0066, 0.2780,\n",
      "         0.7880, 0.0166, 0.3709, 0.5500, 0.8775, 0.9782],\n",
      "        [0.9852, 0.2740, 0.5605, 0.4627, 0.1488, 0.4740, 0.4224, 0.1127, 0.7800,\n",
      "         0.6065, 0.3646, 0.9855, 0.9512, 0.8349, 0.6152, 0.0999, 0.4404, 0.0929,\n",
      "         0.6295, 0.0685, 0.8393, 0.8998, 0.4601, 0.9306]])\n",
      "tensor([[[0.2711, 0.8774, 0.0171, 0.2485],\n",
      "         [0.1644, 0.9675, 0.1289, 0.2879],\n",
      "         [0.1195, 0.3633, 0.2045, 0.3651],\n",
      "         [0.9948, 0.1026, 0.8682, 0.8488],\n",
      "         [0.1908, 0.5127, 0.2769, 0.1463],\n",
      "         [0.2324, 0.4461, 0.0231, 0.6756]],\n",
      "\n",
      "        [[0.0144, 0.5674, 0.1124, 0.1504],\n",
      "         [0.7175, 0.6074, 0.3690, 0.9623],\n",
      "         [0.4468, 0.8777, 0.1288, 0.8800],\n",
      "         [0.9503, 0.4536, 0.5544, 0.0623],\n",
      "         [0.0066, 0.2780, 0.7880, 0.0166],\n",
      "         [0.3709, 0.5500, 0.8775, 0.9782]],\n",
      "\n",
      "        [[0.9852, 0.2740, 0.5605, 0.4627],\n",
      "         [0.1488, 0.4740, 0.4224, 0.1127],\n",
      "         [0.7800, 0.6065, 0.3646, 0.9855],\n",
      "         [0.9512, 0.8349, 0.6152, 0.0999],\n",
      "         [0.4404, 0.0929, 0.6295, 0.0685],\n",
      "         [0.8393, 0.8998, 0.4601, 0.9306]]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "x = torch.rand((3,24))\n",
    "print(x)\n",
    "end=[]\n",
    "for i in range(len(x)):\n",
    "    end.append(torch.stack(list(x[i].split(4, 0))))\n",
    "x=torch.stack(end)\n",
    "print(x)\n",
    "# b0=torch.stack\n",
    "# print(b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs0",
   "language": "python",
   "name": "bs0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
