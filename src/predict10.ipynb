{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "穿着背心的*醒来，看看手机，三点了。\n"
     ]
    }
   ],
   "source": [
    "def read_list(text_path):\n",
    "    lsit=[]\n",
    "    with open('%s' % text_path, 'r', encoding=\"utf8\") as f:  # 打开一个文件只读模式\n",
    "        line = f.readlines()  # 读取文件中的每一行，放入line列表中\n",
    "        for line_list in line:\n",
    "            lsit.append(line_list.replace('\\n',''))\n",
    "    return lsit\n",
    "train_x=read_list('../data/pre_data.txt')\n",
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ban是cls的后缀\n",
    "ban='_1637318143.8513994_2_0.7074_0.7111'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "        self.model.resize_token_embeddings(len(tokenizer))##############\n",
    "        self.model.to(device)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, 6)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "#         print(outputs[0])torch.Size([8, 100, 768])\n",
    "#         print(outputs[1])torch.Size([8, 768])\n",
    "#         print(outputs[0][:,0,:])torch.Size([8, 768])\n",
    "#         print(outputs[1])torch.Size([8, 768])\n",
    "        x = outputs[1]\n",
    "        x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "added_token=['##char##']\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\",additional_special_tokens=added_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda_num = str(6)\n",
    "device_s = \"cuda:\" + cuda_num\n",
    "device = torch.device(device_s if torch.cuda.is_available() else \"cpu\")\n",
    "cls=torch.load(\"../data/cls\"+str(ban)+\".model\",map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "def get_output(text):\n",
    "    with torch.no_grad():\n",
    "        text2id = tokenizer(\n",
    "            text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "    #     print(text,text2id)\n",
    "        input_ids=text2id[\"input_ids\"].to(device)\n",
    "        mask=text2id[\"attention_mask\"].to(device)\n",
    "    #         print(text2id)\n",
    "        output = cls(input_ids, attention_mask=mask)\n",
    "        \n",
    "        output1=[]\n",
    "    \n",
    "        for i in output:#18\n",
    "            s=''\n",
    "\n",
    "            for j in i:\n",
    "                s+=str(j.item())\n",
    "                s+=','\n",
    "            s=s[:-1]\n",
    "            output1.append(s)\n",
    "            \n",
    "    return output1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.03936658799648285,-0.014446040615439415,0.4240415692329407,0.06290137767791748,-0.021756447851657867,0.005224205553531647', '-0.03936658799648285,-0.014446040615439415,0.4240415692329407,0.06290137767791748,-0.021756447851657867,0.005224205553531647']\n"
     ]
    }
   ],
   "source": [
    "text = ['##char##与n3：啊？','##char##与n3：啊？']\n",
    "# 爱、乐、惊、怒、恐、哀\n",
    "print(get_output(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[i.replace('*','##char##') for i in train_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21360:21376,len=16\r"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "\n",
    "i=0\n",
    "batch_size=16\n",
    "while(i<len(train_x)):\n",
    "    if i+batch_size<=len(train_x):\n",
    "        input_x=train_x[i:i+batch_size]\n",
    "        print('{}:{},len={}'.format(i,i+batch_size,batch_size),end = \"\\r\")\n",
    "    else:\n",
    "        input_x=train_x[i:len(train_x)]\n",
    "        print('{}:{},len={}'.format(i,len(train_x)-1,len(train_x)-i),end = \"\\r\")\n",
    "        \n",
    "    pre=get_output(input_x)\n",
    "#     print(pre)\n",
    "    result+=pre\n",
    "#     print(len(result))\n",
    "    i+=batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34170_0002_A_12\n",
      "21376\n",
      "21376\n",
      "-0.03576308861374855,0.01957029104232788,0.009344331920146942,-0.00970818754285574,0.11695418506860733,0.1636660099029541\n"
     ]
    }
   ],
   "source": [
    "end_list=read_list('../data/end_list.txt')\n",
    "print(end_list[0])\n",
    "print(len(result))\n",
    "print(len(end_list))\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = open('../data/result'+str(ban)+'.txt', encoding='utf-8', mode='w')\n",
    "ff.write('id\\temotion\\n')\n",
    "for i in range(len(end_list)):\n",
    "    ff.write(end_list[i]+'\\t'+result[i])\n",
    "    ff.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs0",
   "language": "python",
   "name": "bs0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
